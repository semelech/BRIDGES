var ptx_lunr_search_style = "textbook";
var ptx_lunr_docs = [
{
  "id": "colophon-1",
  "level": "1",
  "url": "colophon-1.html",
  "type": "Colophon",
  "number": "",
  "title": "Colophon",
  "body": "   example.org   https:\/\/example.org   copyright  "
},
{
  "id": "section-Bridges",
  "level": "1",
  "url": "section-Bridges.html",
  "type": "Section",
  "number": "1.1",
  "title": "About BRIDGES",
  "body": " About BRIDGES  BRIDGES is an API\/Toolkit for providing easy-to-use interfaces to real-world data and internet-based information systems, that are exciting and engaging, such as social network data (Reddit), entertainment (movies, songs), encyclopaedia type systems (Wikipedia, Gutenberg books), scientific\/engineering (USGIS Earthquakes) or geographic (city, county, countries, OpenStreet Map, Elevation), entertainment repositories (IMDB, GeniusAPI) data. The BRIDGES toolkit provides a set of classes in C++, Java and Python, to support early CS courses, such CS1\/CS2 and Data Structures\/Algorithm Analysis. See the BRIDGES home page for examples and a short video on an introduction to BRIDGES.  Bridges home page  How does Bridges help:    Provides easy-to-use interfaces to exciting, engaging real-world data (social networks, scientific data, etc), to make it possible for their use in freshmen\/sophomore level CS courses    Makes it easy to visualize course assignments in a CS1, CS2, data structures, or algorithm courses    Is carefully designed to augment the student experience in routine introductory courses in Computer Science    "
},
{
  "id": "sec-Setup",
  "level": "1",
  "url": "sec-Setup.html",
  "type": "Section",
  "number": "1.2",
  "title": "Setting up Bridges",
  "body": " Setting up Bridges  Here is a link to setting up Bridges on your computer in NetBeans: Bridges setup   "
},
{
  "id": "section-program",
  "level": "1",
  "url": "section-program.html",
  "type": "Section",
  "number": "1.3",
  "title": "Hello World program",
  "body": " Hello World program   Simple Java Program  Let's write a simple Java program that prints \"Hello, World!\" to the console:  public class HelloWorld { public static void main(String[] args) { System.out.println(\"Hello, World!\"); } }   "
},
{
  "id": "example-1",
  "level": "2",
  "url": "section-program.html#example-1",
  "type": "Example",
  "number": "1.3.1",
  "title": "Simple Java Program.",
  "body": " Simple Java Program  Let's write a simple Java program that prints \"Hello, World!\" to the console:  public class HelloWorld { public static void main(String[] args) { System.out.println(\"Hello, World!\"); } }  "
},
{
  "id": "sec-SmileyFace",
  "level": "1",
  "url": "sec-SmileyFace.html",
  "type": "Section",
  "number": "1.4",
  "title": "Simple Smiley Face",
  "body": " Simple Smiley Face  Here is a link to a simple smiley face program that you can do once Bridges is setup on your computer: Here   "
},
{
  "id": "section-ITOOP",
  "level": "1",
  "url": "section-ITOOP.html",
  "type": "Section",
  "number": "2.1",
  "title": "Introduction to Object-oriented Programming",
  "body": " Introduction to Object-oriented Programming  Object-oriented programming (OOP) is a programming paradigm based on the concept of objects, which are data structures that contain data, in the form of fields (or attributes) and code, in the form of procedures, (or methods). A distinguishing feature of objects is that an object’s procedures provide access to and modify its fields.  In object-oriented programming, computer programs are designed by making them out of objects that interact with one another. There is significant diversity in object-oriented programming, but most popular languages are class-based, meaning that objects are instances of classes, which typically also determines their type.  Object orientation is an outgrowth of procedural programming. Procedural programming is a programming paradigm, derived from structured programming, based upon the concept of the procedure call. Procedures, also known as routines, subroutines, or methods define the computational steps to be carried out.  Any given procedure might be called at any point during a program’s execution, including by other procedures or itself. Procedural programming is a list or set of instructions telling a computer what to do step by step and how to perform from the first code to the second code. Procedural programming languages include C, Fortran, Pascal, and BASIC.  The focus of procedural programming is to break down a programming task into a collection of variables, data structures, and subroutines, whereas in object-oriented programming it is to break down a programming task into objects that expose behavior (methods) and data (fields) using interfaces. The most important distinction is that while procedural programming uses procedures to operate on data structures, object-oriented programming bundles the two together, so an object, which is an instance of a class, operates on its “own” data structure.  "
},
{
  "id": "sec-POOOP",
  "level": "1",
  "url": "sec-POOOP.html",
  "type": "Section",
  "number": "2.2",
  "title": "Principles of Object-oriented Programming",
  "body": " Principles of Object-oriented Programming   Encapsulation  Encapsulation refers to the creation of self-contained modules (classes) that bind processing functions to its data members. The data within each class is kept private. Each class defines rules for what is publicly visible and what modifications are allowed.    Inheritancr  Classes may be created in hierarchies, and inheritance lets the structure and methods in one class pass down the class hierarchy. By inheriting code, complex behaviors emerge through the reuse of code in a parent class. If a step is added at the bottom of a hierarchy, only the processing and data associated with that unique step must be added. Everything else above that step may be inherited. Reuse is considered a major advantage of object orientation.    Polymorphism  Object oriented programming lets programmers create procedures for objects whose exact type is not known until runtime. For example, a screen cursor may change its shape from an arrow to a line depending on the program mode. The routine to move the cursor on screen in response to mouse movement can be written for “cursor”, and polymorphism lets the right version for the given shape be called.    Abstraction  An abstraction denotes the essential characteristics of an object that distinguish it from all other kinds of objects and thus provide crisply defined conceptual boundaries, relative to the perspective of the viewer. [Booch] Abstraction denotes a model, a view, or some other focused representation for an actual item. It’s the development of a software object to represent an object we can find in the real world. Encapsulation hides the details of that implementation.   "
},
{
  "id": "sec-UML",
  "level": "1",
  "url": "sec-UML.html",
  "type": "Section",
  "number": "2.3",
  "title": "The Unified Modeling Language",
  "body": " The Unified Modeling Language  The Unified Modeling Language, or UML, is an industry standard graphical notation for describing and analysing software designs. The symbols and graphs used in the UML are an outgrowth of efforts in the 1980’s and early 1990’s to devise standards for Computer-Aided Software Engineering (CASE). UML represents a unification of these efforts. In 1994 - 1995 several leaders in the development of modeling languages, Grady Booch, Ivar Jacobson, and James Rumbaugh, attempted to unify their work. To eliminate the method fragmentation that they concluded was impeding commercial adoption of modeling tools, they developed UML, which provided a level playing field for all tool vendors.  UML has been accepted as a standard by the Object Management Group (OMG). The OMG is a non-profit organization with about 700 members that sets standards for distributed object-oriented computing.  UML was initially largely funded by the employer of Booch, Jacobson and Rumbaugh, aka the three amigos, Rational Software, which was sold to IBM in 2002.  A software model is any textual or graphic representation of an aspect of a software system. This could include requirements, behavior, states or how the system is installed. The model is not the actual system, rather it describes different aspects of the system to be developed. UML defines a set of diagrams and corresponding rules that can be used to model a system. The diagrams in the UML are generally divided into two broad categories or views, static and dynamic.  This course does not provide anywhere near a comprehensive review of the UML. The intent is to introduce you to the basics you need to understand the designs presented in this course. Since there is an excellent chance you will encounter the UML or something very similar to it in your professional career and the diagrams used in this course are used not only in the UML, but in other modeling systems as well.  "
},
{
  "id": "sec-Testing",
  "level": "1",
  "url": "sec-Testing.html",
  "type": "Section",
  "number": "2.4",
  "title": "Testing vs Debugging",
  "body": " Testing vs Debugging  When we “write a program”, we actually spend most of our time testing and debugging. These are two separate things. Testing refers to determining whether the program operates as we intend. Debugging refers to correcting the program once we determine that it is not operating as we intend. So we can only debug to the extent that we have tested and determined that there is a problem that needs to be corrected. Debugging to fix a known problem can sometimes be extremely hard, but is often somewhat mechanical. Testing requires a lot of skill and empathy, in order to think of all of the ways that a program might go wrong (in particular, all of the input paths to the program that might affect its behavior).  "
},
{
  "id": "sec-UnitTest",
  "level": "1",
  "url": "sec-UnitTest.html",
  "type": "Section",
  "number": "2.5",
  "title": "Unit Testing",
  "body": " Unit Testing   Why do we need unit testing?  Software evolves over time. Thus, we should adapt to change rather than stick to a strict plan. Software development is a collaborative process: many individuals work on different parts of the software to build something that meets customer needs, and different individuals perform different roles iteratively to determine a product's future. We work with new technologies, ever-changing requirements, people movement, or a combination of these. All of these determine that software projects involve a lot of uncertainties. People say that the only constant is uncertainty. To overcome the fear and manage these uncertainties, we need a software development practice that can help us produce working software. It must keep things simple and provide us quick feedback in case things go wrong. We need to verify the working software when change happens.  Test-driven development (TDD) is one of the practices of Agile software development that a lot of developers use in some shape or form. The premise of TDD is that you write a failing test case before you write the production code itself. TDD, if done correctly, can help you write software that meets customer expectations, has a simple design, and has fewer defects.    What is Unit Testing?  The main idea of unit testing is testing software with a small piece of source code (unit, component, and\/or function) of the same software. \"Unit testing\" means that the software consists of \"units\" which are separate testable parts of the product. An individual program, class, method, function etc. can be such \"unit\". Unit testing allows checking whether a unit behaves as the developer intended and whether a unit corresponds to the design specifications. Unit testing provides an ability of independent testing for each software unit.  The advantages of unit testing is as follows:    Developers looking to learn what functionality is provided by a unit and how to use it can look at the unit tests to gain a basic understanding of the unit API.    Unit testing allows the programmer to refactor code at a later date, and make sure the module still works correctly (i.e. Regression testing).    The procedure is to write test cases for all functions and methods so that whenever a change causes a fault, it can be quickly identified and fixed. Due to the modular nature of the unit testing, we can test parts of the project without waiting for others to be completed.      What is JUnit?  JUnit, developed by Kent Beck and Erich Gamma, is one of the most popular unit-testing frameworks for Java developers. It was originally based on SUnit, a unit-testing framework written in Smalltalk (developed by Kent Beck). The first version of JUnit was released in 1997.  Prior to the introduction of JUnit, testing discipline was dominated by capture and replay testing tools. These tools were black-box testing tools, which used to capture the state of the system with a given input and then try to replay it. The tests written in such a framework involved an enormous amount of effort. These tools were not designed to unit test a component as they tested the application using its graphical user interface (GUI).  JUnit rejected the idea of GUI-based tests. It instead provided a lightweight framework, which enabled test creation by writing code in Java. This allowed developers to build test suites for every piece of their code. Due to its benefits, JUnit was integrated with all kinds of build tools and integrated development environments (IDEs).  JUnit is a unit testing framework for Java programming language. It plays a crucial role test-driven development, and is a family of unit testing frameworks collectively known as xUnit. JUnit promotes the idea of \"first testing then coding\", which emphasizes on setting up the test data for a piece of code that can be tested first and then implemented. This approach is like \"test a little, code a little, test a little, code a little.\" It increases the productivity of the programmer and the stability of program code, which in turn reduces the stress on the programmer and the time spent on debugging.   "
},
{
  "id": "sec-JUintExample",
  "level": "1",
  "url": "sec-JUintExample.html",
  "type": "Section",
  "number": "2.6",
  "title": "JUint Example",
  "body": " JUint Example   Let us assume that we have a single class, Calculator, in a class library project. Add looks pretty reliable at first glance, but so does all the code you write. You still need to test it, if only to prove your rightness to others.   public class Calculator{ public int Add(int x, int y){ return x + y; } }  public class Calculator{  public int Add(int x, int y){  return x + y;  }  }   Traditionally, we write a console class or main method inside the class, for example,   class CalculatorTest { static void Main(string[] args){ Calculator calculator = new Calculator(); int result = calculator.Add(5, 6); if (result != 11) throw new InvalidOperationException(); } }  class CalculatorTest {  static void Main(string[] args){  Calculator calculator = new Calculator();  int result = calculator.Add(5, 6);  if (result != 11)  throw new InvalidOperationException();  }  }   By using Junit package, we could have:   class CalculatorTest{ public void testAdd(){ Calculator calculator = new Calculator(); assertTrue(11 == calculator.Add(5, 6)); } }  class CalculatorTest{  public void testAdd(){  Calculator calculator = new Calculator();  assertTrue(11 == calculator.Add(5, 6));  }  }   Note:    The key part here is writing a failing test first.    Remember that it is our objective to work in small steps to make sure we will never experience a sudden and unexpected test success or test failure. In practice, most developers take increasingly bigger steps, with the result of frequent surprises when they run their tests. That's where we will hopefully distinguish ourselves as test-first masters by making giant steps when moving in well-known territory and making very tiny forward and sideward steps in unknown terrain and on slippery ground. However, this strategy requires that we know how to make the tiny steps in the first place.    At the beginning of each step, there was a test which was directly or indirectly motivated by the requirements specification. To be able to write this test, we had to make decisions about the public interface desired for our OUT (object under test). This public interface served both to “stimulate” the test object and to verify the correct behavior.    This approach drove and controlled the development of our production code from the tests.    So far, our tests have concentrated exclusively on the externally visible behavior of the OUT. In Java this theoretically includes all methods (and variables) with public, protected, or package scope visibility. In practice we restrict ourselves to use only what is intended to be used from the outside, that is from client code, which usually leaves out protected methods and members available for subclassing.     "
},
{
  "id": "sec-GameTutorial",
  "level": "1",
  "url": "sec-GameTutorial.html",
  "type": "Section",
  "number": "2.7",
  "title": "Bridges Game API Tutorial",
  "body": " Bridges Game API Tutorial  Here is a link to starting out Bridges Game API: Here. It is a simple tutorial to get you started on working with Bridges Game API.  "
},
{
  "id": "sec-DSAlgor",
  "level": "1",
  "url": "sec-DSAlgor.html",
  "type": "Section",
  "number": "3.1",
  "title": "Data Structures and Algorithms",
  "body": " Data Structures and Algorithms   Introduction  To determine the number of cities with a population exceeding 250,000 within a 500-mile radius of Dallas, Texas; to identify the count of employees in my company earning over $100,000 per year; or to assess the feasibility of connecting all telephone customers with less than 1,000 miles of cable, it is insufficient to possess the necessary information alone. We must organize that information in a manner that enables us to promptly find the answers to meet our requirements.  Information representation lies at the core of computer science. Most computer programs are designed not primarily for performing calculations, but for storing and retrieving information—usually with optimal speed. Hence, the study of data structures and the algorithms that manipulate them forms the essence of computer science. This book aims to assist you in comprehending how to structure information to support efficient processing.  In any course on Data Structures and Algorithms, you will typically encounter three key aspects:    Introduction to commonly used data structures and algorithms, which constitute a programmer's fundamental toolkit. For many problems, a data structure or algorithm from this toolkit will provide a suitable solution. Our focus is on data structures and algorithms that have stood the test of time and proven to be most beneficial.    Exploration of tradeoffs and reinforcement of the idea that every data structure or algorithm entails costs and benefits. This is achieved by describing the space and time requirements associated with typical operations for each data structure. Similarly, we examine the time needed for various input types in each algorithm.    Instruction on measuring the effectiveness of a data structure or algorithm. Only through such evaluation can we determine which data structure from our toolkit is most suitable for a new problem. The techniques presented also enable us to assess the merits of new data structures that we or others might invent.    When approaching problem-solving, there are often multiple approaches available. How do we choose among them? At the core of computer program design lie two goals, which can sometimes conflict:    Designing an algorithm that is easy to understand, code, and debug.    Designing an algorithm that efficiently utilizes the computer's resources.    Ideally, a resulting program embodies both of these goals. We could consider such a program to be \"elegant.\" While the algorithms and code examples presented here strive for elegance in this sense, it is not the primary objective of this book to explicitly address issues related to goal. Such concerns primarily fall within the domain of Software Engineering. Instead, our focus primarily revolves around issues related to goal.  How do we measure efficiency? The method employed to evaluate the efficiency of an algorithm or computer program is known as asymptotic analysis. This analysis also allows us to define the inherent complexity of a problem. Throughout the book, we employ asymptotic analysis techniques to estimate the time cost for each algorithm presented. This enables you to compare the efficiency of different algorithms used to solve the same problem and understand how they fare against one another.    Philosophy of Data Structures  You might assume that as computers become more powerful, the importance of program efficiency diminishes. After all, processor speed and memory capacity continue to improve. Can't we rely on tomorrow's hardware to solve today's efficiency issues?  However, our history of computer development has shown that as computing power increases, we tend to tackle increasingly complex problems. This can take the form of more sophisticated user interfaces, larger problem sizes, or previously unsolvable computational challenges. With more complex problems, the demand for computation grows, amplifying the need for efficient programs. Unfortunately, as tasks become more intricate, they often deviate from our everyday experiences. Therefore, today's computer scientists must be well-versed in the principles of efficient program design because their everyday intuitions may not directly apply in the realm of computer programming.  In a broad sense, a data structure encompasses any representation of data and the operations associated with it. Even a simple integer or floating-point number stored in a computer can be seen as a basic data structure. However, the term \"data structure\" commonly refers to the organization or structuring of a collection of data items. For instance, a sorted list of integers stored in an array exemplifies such a structuring. These concepts are further explored in the discussion of Abstract Data Types.  With sufficient space to store a collection of data items, it is always possible to search for specific items, process the items in any desired order, or modify the value of individual items. The most straightforward example is an unsorted array containing all the data items. It is possible to perform all necessary operations on such an unsorted array. However, employing the appropriate data structure can make a significant difference in program efficiency. For instance, searching for a specific record in a hash table is much faster than searching for it in an unsorted array.  An efficient solution is one that solves the problem while adhering to the required resource constraints. These constraints can include the available space to store data (which may involve separate constraints for main memory and disk space) and the allotted time for each subtask. Sometimes, a solution is deemed efficient if it utilizes fewer resources than known alternatives, regardless of meeting specific requirements. The cost of a solution refers to the amount of resources it consumes. Typically, cost is measured in terms of a key resource, such as time, assuming that the solution fulfills other resource constraints as well.    Selecting a Data Structure  It is an essential reminder that people develop programs to solve problems, yet programmers sometimes overlook this fact. Therefore, it is crucial to always bear this truth in mind when choosing a data structure to tackle a specific problem. The first step towards selecting the right data structure is analyzing the problem and determining the performance objectives that must be met. Without this initial analysis, program designers often make the mistake of using a familiar but inappropriate data structure, resulting in a slow program. Conversely, there is no point in adopting a complex representation to \"enhance\" a program that can achieve its performance goals through a simpler design.  To select a data structure for problem-solving, follow these steps:    Analyze the problem and identify the essential operations that the data structure must support. These operations may include inserting a data item into the structure, deleting a data item, and finding a specific data item.    Quantify the resource limitations for each operation.    Choose the data structure that best satisfies the requirements identified in the previous steps.    This three-step approach to data structure selection operationalizes a data-centric perspective in the design process. The primary focus is on the data and the operations performed on them. The next concern is finding the appropriate representation for the data, followed by implementing that representation.  Resource constraints, especially for critical operations like searching, inserting, and deleting data records, typically guide the selection of a data structure. Addressing questions regarding the relative significance of these operations can help in making informed decisions. Whenever you face the task of choosing a data structure, consider asking yourself the following three questions:    Are all data items inserted into the structure at the beginning, or are insertions interspersed with other operations? Static applications, where data is loaded initially and remains unchanged, often benefit from simpler data structures for efficient implementation. In contrast, dynamic applications frequently require more intricate structures.    Is it possible to delete data items? If so, this may introduce additional complexity into the implementation.    Are all data items processed in a defined order, or does the search involve locating specific data items? \"Random access\" searches typically necessitate more complex data structures.    By carefully considering these factors and evaluating the requirements of your problem, you can make informed choices when selecting an appropriate data structure.   "
},
{
  "id": "sec-AbstractDT",
  "level": "1",
  "url": "sec-AbstractDT.html",
  "type": "Section",
  "number": "3.2",
  "title": "Abstract Data Types",
  "body": " Abstract Data Types   Abstract Data Types  This module introduces terminology and definitions related to techniques used in managing the complexity of computer programs. It provides clear explanations for the fundamental yet sometimes elusive terms \"data item\" and \"data structure.\" We begin by discussing the basic building blocks on which data structures are constructed.  A type represents a set of values. For instance, the Boolean type consists of the values true and false, while the integer type encompasses whole numbers. An integer is considered a simple type since its values have no internal components. On the other hand, a bank account record typically comprises various pieces of information like name, address, account number, and balance. Such a record exemplifies an aggregate type or composite type. A data item refers to a piece of information or a record whose value is drawn from a particular type. We say that a data item belongs to a type.  A data type is a type that comes with a collection of operations used to manipulate it. For example, an integer variable belongs to the integer data type, and addition is one operation applicable to the integer data type.  It is important to distinguish between the logical concept of a data type and its physical implementation in a computer program. For instance, there exist two traditional implementations for the list data type: the linked list and the array-based list. This means that a list data type can be implemented using either a linked list or an array. However, when we are working with a more complex design and need to utilize a list, we do not necessarily require knowledge of how it is implemented. For example, a list might be used to aid in implementing a graph data structure.  Similarly, the term \"array\" can refer to both a data type and an implementation. In computer programming, \"array\" commonly denotes a contiguous block of memory locations, where each location stores a fixed-length data item. In this sense, an array is a physical data structure. However, \"array\" can also denote a logical data type consisting of a collection of data items, typically of the same type, each identified by an index number. Arrays can be implemented in various ways beyond a contiguous block of memory locations. For instance, a sparse matrix is a large two-dimensional array that only stores a relatively small number of non-zero values. It can be implemented using a linked structure or a hash table, while still providing the same interface as if it were implemented as a block of contiguous memory locations using traditional row and column indices.  An abstract data type (ADT) specifies a data type within a particular programming language, independent of any specific implementation. The ADT interface is defined in terms of a type and a set of operations associated with that type. The behavior of each operation is determined by its inputs and outputs. An ADT does not dictate how the data type should be implemented. The implementation details are hidden from the ADT's user, ensuring encapsulation and protecting them from external access.  A data structure represents the concrete implementation of an ADT. In object-oriented languages, an ADT and its implementation together form a class. Each operation associated with the ADT is implemented as a member function or method. The variables that define the storage space required by a data item are known as data members. An object is an instance of a class, meaning it is created and occupies memory during program execution.  The term \"data structure\" often refers to data stored in a computer's main memory, while \"file structure\" typically describes the organization of data on peripheral storage devices like disk drives or CDs.   "
},
{
  "id": "sec-ADTGenerics",
  "level": "1",
  "url": "sec-ADTGenerics.html",
  "type": "Section",
  "number": "3.3",
  "title": "Abstract Data Types and Generics",
  "body": " Abstract Data Types and Generics   Algorithms  At the heart of computer program design are two (sometimes conflicting) goals:    To design an algorithm that is easy to understand, code, and debug.    To design an algorithm that makes efficient use of the computer's resources.    The method for evaluating the efficiency of an algorithm or computer program is called asymptotic analysis.      Guideline of Data Structure Selection  Here is a link to a Powerpoint on the guidlines to data structures: Powerpoint    "
},
{
  "id": "sec-PatternTutorial",
  "level": "1",
  "url": "sec-PatternTutorial.html",
  "type": "Section",
  "number": "3.4",
  "title": "Pattern Tutorial",
  "body": " Pattern Tutorial  Here is a link to a simple Bridges pattern program: Here   "
},
{
  "id": "sec-PAP",
  "level": "1",
  "url": "sec-PAP.html",
  "type": "Section",
  "number": "4.1",
  "title": "Problems, Algorithms, and Programs",
  "body": " Problems, Algorithms, and Programs   Problems  Programmers regularly encounter three distinct concepts: problems, algorithms, and computer programs.  A problem can be understood as a task that needs to be accomplished, typically described in terms of inputs and desired outputs. It is crucial to define a problem precisely, focusing on inputs and matching outputs, without specifying the solution method. Constraints on the resources allowable for a solution should be included in the problem definition. Every problem solvable by a computer is subject to such constraints, whether explicitly stated or implied. For instance, a computer program can only utilize available main memory and disk space, and it should execute within a reasonable timeframe.  Problems can be seen as mathematical functions. A function matches inputs (domain) with corresponding outputs (range). Inputs can be individual values or collections of information, referred to as parameters. A specific set of parameter values represents an instance of the problem. For example, a sorting function's input parameter might be an array of integers, and a particular array with its size and values at each position would be an instance of the sorting problem. Different instances may yield the same output, but any instance of a problem consistently produces the same output whenever the function is computed with that particular input.  This perspective of problems behaving like mathematical functions may differ from your intuition about how computer programs operate. You may be aware of programs where the same input value produces different outputs on separate occasions. For example, entering the \"date\" command in a typical Linux command line prompt provides the current date, which naturally changes with each day. However, there is more to the input of the \"date\" program than just the command entered. The program computes a function, and on any given day, a properly functioning \"date\" program with a fully specified input will always produce a single answer. The output of any computer program is entirely determined by the program's complete set of inputs. Even a \"random number generator\" is entirely determined by its inputs (although some random number generation systems seem to circumvent this by incorporating random inputs from external physical processes beyond the user's control). The scope of functions implementable by programs falls within the realm of Computability.    Algorithms  An algorithm refers to a method or process employed to solve a problem. If we consider the problem as a function, an algorithm serves as an implementation of that function, transforming an input into the corresponding output. Multiple algorithms can be employed to solve a single problem. Conversely, a specific algorithm is designed to solve only one problem, computing a particular function. The OpenDSA modules cover various problems, and for many of them, we will explore multiple algorithms. In the case of sorting, there are over a dozen well-known algorithms.  Having knowledge of multiple solutions for a problem offers the advantage of selecting the most efficient solution for a particular problem variation or class of inputs. Solution A may be more efficient than solution B for a specific problem variation or input class, while solution B may be superior for another variation or input class. For instance, one sorting algorithm may be ideal for sorting a small collection of integers (which is significant when performing the task multiple times), while another algorithm may excel at sorting a large collection of integers. A third algorithm might be the most suitable for sorting a collection of variable-length strings.  By definition, something can only be considered an algorithm if it possesses the following properties:    Correctness: It accurately computes the desired function, transforming each input into the correct output. Note that every algorithm implements a function, as it maps every input to some output (even if that output is a program crash). The concern here is whether a given algorithm effectively implements the intended function.    Concrete Steps: It comprises a series of well-defined and comprehensible steps that can be executed by the person or machine performing the algorithm. Each step must be accomplishable within a finite amount of time. Thus, the algorithm provides a step-by-step \"recipe\" for problem-solving, where each step is within our capability to perform. The feasibility of executing a step may vary depending on the intended executor. For example, the steps in a cookbook recipe may be sufficiently concrete for instructing a human cook but not for programming an automated cookie-making factory.    Unambiguous Execution: There should be no ambiguity about which step is to be executed next. Typically, algorithms involve selection, such as the use of an \"if\" statement, enabling a choice of the next step. However, at the time of making the choice, the selection process must be unambiguous.    Finite Steps: It consists of a finite number of steps. If the algorithm's description were to comprise an infinite number of steps, it would be impossible to document or implement it as a computer program. Most algorithm description languages incorporate iteration constructs, allowing repeated actions. Programming languages feature constructs like the \"while\" and \"for\" loops to facilitate iteration. Iteration enables concise descriptions, with the actual number of steps executed controlled by the input.    Termination: It must terminate and not enter an infinite loop, ensuring that the algorithm reaches a conclusion.      Programs  We often perceive a computer program as a specific instance or tangible representation of an algorithm in a programming language. Algorithms are typically described in terms of programs or sections of programs. Naturally, multiple programs can be instances of the same algorithm since various modern programming languages can be used to implement the same set of algorithms (although some languages may offer more programmer-friendly features). In practice, the terms \"algorithm\" and \"program\" are often used interchangeably, even though they represent distinct concepts. However, by definition, an algorithm must provide sufficient detail to allow its conversion into a program when necessary.  The requirement for an algorithm to terminate means that not all computer programs align with the technical definition of an algorithm. An example is your operating system. Nonetheless, it is possible to consider the different tasks performed by an operating system, each with its associated inputs and outputs, as individual problems. These problems can be addressed by specific algorithms implemented within different parts of the operating system program. Each algorithm terminates once it produces the desired output for its respective problem.   "
},
{
  "id": "sec-BWAvgCases",
  "level": "1",
  "url": "sec-BWAvgCases.html",
  "type": "Section",
  "number": "4.2",
  "title": "Best, Worst, and Average Cases",
  "body": " Best, Worst, and Average Cases  When analyzing an algorithm, we often contemplate whether to study its best, worst, or average case. Typically, the best case is not of great interest since it tends to occur rarely and may present an overly optimistic depiction of the algorithm's runtime. Analyzing the best case is not usually representative of the algorithm's behavior. However, there are exceptional scenarios where a best-case analysis proves valuable, especially when the best case is highly likely to occur. For instance, the Shellsort and Quicksort algorithms leverage the best-case running time of Insertion Sort to enhance their efficiency.  On the other hand, analyzing the worst case has its advantages as it guarantees a lower bound on the algorithm's performance. This becomes particularly crucial for real-time applications like air traffic control systems. In such cases, it is unacceptable to use an algorithm that performs efficiently for most situations but fails to meet performance requirements when faced with specific scenarios, such as when all airplanes approach from the same direction.  However, for various applications, especially when considering the cumulative cost of executing the program on multiple inputs, analyzing the worst case may not be an accurate measure of the algorithm's overall performance. In such instances, the preferred approach is often to examine the average-case running time. This allows us to understand the algorithm's typical behavior when handling inputs of size n. Nevertheless, conducting an average-case analysis is not always feasible. It requires a thorough understanding of how the actual inputs and their associated costs are distributed among all possible inputs to the program. For example, it was mentioned previously that the average-case performance of the sequential search algorithm involves examining half of the array values. However, this holds true only if the element with value K is equally likely to appear in any position within the array. If this assumption is incorrect, the algorithm may not examine half of the array values on average.  The characteristics of the data distribution significantly impact various search algorithms, including hashing and search trees such as Binary Search Trees (BST). Incorrect assumptions about the data distribution can lead to detrimental effects on a program's space or time performance. Conversely, unusual data distributions can sometimes be leveraged advantageously, as demonstrated by self-organizing lists.  In summary, for real-time applications, a worst-case analysis is often preferred. Otherwise, if sufficient knowledge about the input distribution is available, an average-case analysis is desirable. In the absence of such knowledge, resorting to a worst-case analysis becomes necessary.  "
},
{
  "id": "sec-ComputerorAlgorithm",
  "level": "1",
  "url": "sec-ComputerorAlgorithm.html",
  "type": "Section",
  "number": "4.3",
  "title": "Faster Computer, or Faster Algorithm",
  "body": " Faster Computer, or Faster Algorithm  Imagine you are faced with a problem and you have an algorithm with a running time proportional to n^2, where n represents the size of the input. However, the resulting program takes ten times longer to run than desired. If you were to replace your current computer with a new one that is ten times faster, would the n^2 algorithm become acceptable? It might seem logical that the faster computer would allow you to complete your work quickly enough, even with an algorithm that has a high growth rate, as long as the problem size remains the same. But here's an interesting observation: when most people acquire a faster computer, they don't necessarily run the same problem faster; they tend to tackle a larger problem instead. For example, on your old computer, you might have been content sorting 10,000 records during your lunch break. With the new, faster computer, you might aim to sort 100,000 records in the same time. Since you won't finish your lunch any sooner, it makes sense to solve a larger problem. Additionally, due to the increased speed of the new machine, you would ideally like to sort ten times as many records.  If the algorithm's growth rate is linear, meaning the equation describing the running time on input size n is T(n) = cn for some constant c, then sorting 100,000 records on the new machine would take the same amount of time as sorting 10,000 records on the old machine. However, if the algorithm's growth rate exceeds cn, such as c1n^2, then it won't be possible to solve a problem ten times the size in the same amount of time on a machine that is only ten times faster.  "
},
{
  "id": "sec-AALecture",
  "level": "1",
  "url": "sec-AALecture.html",
  "type": "Section",
  "number": "4.4",
  "title": "Algorithm Analysis Lecture",
  "body": " Algorithm Analysis Lecture  Here is the link to download the lecture: Slides   "
},
{
  "id": "sec-ComplexityMatters",
  "level": "1",
  "url": "sec-ComplexityMatters.html",
  "type": "Section",
  "number": "4.5",
  "title": "Complexity Matters",
  "body": " Complexity Matters  Here is a link to an assignment to help you understand Big-Oh notation: Here   "
},
{
  "id": "sec-Pointers",
  "level": "1",
  "url": "sec-Pointers.html",
  "type": "Section",
  "number": "5.1",
  "title": "Pointers Syntax",
  "body": " Pointers Syntax   Syntax  The fundamental concepts of references, pointees, dereferencing, and assigning discussed earlier are all that you require to construct reference code. However, when discussing reference code, we need to employ a familiar syntax, which may not be particularly engaging. In this case, we will utilize the Java language syntax, which offers the advantage of influencing the syntaxes of numerous other programming languages.    Declaring a Reference Variable  To establish a reference to a non-primitive data type (i.e., an object), one must declare a variable of that specific object's type. Similar to other variables, reference variables are declared in the same manner. The declaration includes specifying the type and identifier of the new variable, and memory is allocated to store its value. However, it is crucial to note that declaring a reference variable does not automatically assign a pointee (object) to the reference. Initially, the reference holds a \"bad\" value until it is assigned to an actual object.    Reference Rules  The rules governing reference structures remain concise, regardless of their complexity. A reference variable serves as a container for storing a reference to its associated pointee. The pointee itself holds valuable information or data. To access the content of a reference, the dereference operation is used. However, it is crucial to note that a reference can only be dereferenced once it has been assigned to a valid pointee. Many issues or errors related to references occur when this rule is violated. Allocating a reference does not automatically assign it to refer to a specific pointee. The act of assigning the reference to a particular pointee is a distinct operation that can sometimes be overlooked. When two references are assigned to each other, they start referring to the same pointee. This allows for object sharing, enabling multiple references to access and manipulate the same underlying object.    Java Reference Vs Pointers  Java references possess two key features that distinguish them from the less permissive pointers found in languages like C or C++.    Reduced bugs: Java's implementation of reference manipulation is accurate and automatic, resulting in fewer common reference-related bugs. Additionally, the Java runtime system verifies each reference value whenever it is used. As a result, if a null reference is dereferenced, the system promptly detects it on the line where the issue occurs. This stands in contrast to languages like C++ where dereferencing a null value might not cause a program crash until later stages. This ability to pinpoint problems immediately can greatly enhance a programmer's productivity by identifying the exact location of an issue.    Slower execution: Due to the extensive runtime implementation of reference management and the additional runtime checking, Java code tends to run slower compared to languages like C and C++. However, the advantages of improved programmer efficiency and reduced bugs make the trade-off of slower execution worthwhile for many applications.     "
},
{
  "id": "sec-LinkedNodes",
  "level": "1",
  "url": "sec-LinkedNodes.html",
  "type": "Section",
  "number": "5.2",
  "title": "Linked Nodes",
  "body": " Linked Nodes  In this module, we will introduce the concept of a link node. A link node consists of a value field and a pointer to another link node. In subsequent lessons, you will delve into linked lists, which are constructed using link nodes. However, for now, we will utilize link nodes as a straightforward means to connect various objects together.  "
},
{
  "id": "sec-DoubleLL",
  "level": "1",
  "url": "sec-DoubleLL.html",
  "type": "Section",
  "number": "5.3",
  "title": "Doubly Linked Lists",
  "body": " Doubly Linked Lists  The singly linked list allows direct access from a list node solely to the next node in the list. On the other hand, a doubly linked list offers the convenience of accessing both the next and preceding nodes from a list node. This is achieved by including two pointers in each doubly linked list node: one pointing to the next node (similar to the singly linked list) and another pointing to the preceding node. The primary advantage of using a doubly linked list is its ease of implementation compared to a singly linked list. Although the code for the doubly linked implementation may be slightly longer than its singly linked counterpart, it tends to be more straightforward in its purpose, making it easier to implement and debug. It is important to note that the decision to use a doubly or singly linked list should be hidden from the user of the List class.  Similar to the implementation of our singly linked list, the doubly linked list implementation incorporates a header node. Additionally, we introduce a tailer node at the end of the list. The tailer node, like the header node, does not contain any value and is always present. During the initialization of the doubly linked list, the header and tailer nodes are created. The data member \"head\" points to the header node, and \"tail\" points to the tailer node. These nodes simplify the insert, append, and remove methods by eliminating the need for special-case code when the list is empty or when inserting at the head or tail of the list.  "
},
{
  "id": "sec-LLLecture",
  "level": "1",
  "url": "sec-LLLecture.html",
  "type": "Section",
  "number": "5.4",
  "title": "Linked List Lecture",
  "body": " Linked List Lecture  Here is the link to download the lecture on linked lists: Slides   "
},
{
  "id": "sec-LLTutorial",
  "level": "1",
  "url": "sec-LLTutorial.html",
  "type": "Section",
  "number": "5.5",
  "title": "Linked List Tutorial",
  "body": " Linked List Tutorial  Here is a link to a tutorial on building a linked list: Here   Here is a tutorial on styling a linked list of earthquake records: Here   "
},
{
  "id": "sec-Exercises",
  "level": "1",
  "url": "sec-Exercises.html",
  "type": "Section",
  "number": "5.6",
  "title": "Exercises",
  "body": " Exercises   Linked Lists Vs Array Lists Exercise 1  The assignment is to log the performance of insertion\/deletion operations on ArrayLists and LinkedLists. Complete each unfinished function that has a TODO comment. Please provide comments on how your code works. Here is the main file: main.java   This project uses the built in ArrayList and LinkedList classes in Java.  You can refer to the ArrayList docs: Here   You can refer to the LinkedList docs: Here     Linked Lists Vs Array Lists Exercise 2  Please download this word document: Data_Structure_Operations.docx   Fill out each cell with the BigO worst case runtime complexity of each operation for the given data structure. Also, write a couple sentences describing why each operation is the runtime you have choosen.   "
},
{
  "id": "sec-StackTerm",
  "level": "1",
  "url": "sec-StackTerm.html",
  "type": "Section",
  "number": "6.1",
  "title": "Stack Terminology and Implementation",
  "body": " Stack Terminology and Implementation  The stack represents a structure similar to a list, where elements can only be inserted or removed from one end. Although this limitation makes stacks less flexible compared to lists, it also makes them highly efficient and easy to implement for the operations they support. Many applications only require the limited set of insert and remove operations provided by stacks. In such cases, using the simpler stack data structure is more efficient than using a generic list. For instance, a freelist can be implemented using a stack.  Despite their restrictions, stacks find numerous applications, leading to the development of specialized terminology associated with stacks. Accountants have been using stacks long before computers were invented, referring to them as \"LIFO\" lists, which stands for \"Last-In, First-Out.\" It is important to note that the LIFO policy of stacks means that elements are removed in the reverse order of their arrival.  The element that is accessible and can be manipulated on top of the stack is referred to as the top element. Elements are not inserted into a stack; rather, they are pushed onto the stack. Similarly, when an element is removed from a stack, it is popped. Here is a simple abstract data type (ADT) for a stack.  "
},
{
  "id": "sec-LinkedStacks",
  "level": "1",
  "url": "sec-LinkedStacks.html",
  "type": "Section",
  "number": "6.2",
  "title": "Linked Stacks",
  "body": " Linked Stacks  The linked stack implementation is quite simple. Elements are inserted and removed only from the head of the list. A header node is not used because no special-case code is required for lists of zero or one elements.  Both the array-based and linked implementations of stacks offer constant-time operations, resulting in similar time efficiency. Therefore, neither approach has a significant advantage in terms of time efficiency. However, a comparison can be made based on the total space required by each implementation, similar to the analysis performed for list implementations.  In the case of the array-based stack, a fixed-size array needs to be declared initially, and some space is wasted whenever the stack is not full. On the other hand, the linked stack has the flexibility to shrink and grow dynamically but incurs the overhead of a link field for every element.  When implementing multiple stacks, there is a possibility to leverage the one-way growth characteristic of the array-based stack by using a single array to store two stacks. Each stack grows inward from opposite ends, as illustrated in the figure below, which can help minimize wasted space. However, this approach works best when the space requirements of the two stacks are inversely correlated. Ideally, when one stack grows, the other should shrink. This technique proves particularly effective when elements are transferred between the two stacks. In contrast, if both stacks grow simultaneously, the free space in the middle of the array will be depleted rapidly.  "
},
{
  "id": "sec-StacksLecture",
  "level": "1",
  "url": "sec-StacksLecture.html",
  "type": "Section",
  "number": "6.3",
  "title": "Stacks and Queues Lecture",
  "body": " Stacks and Queues Lecture  Here is the link to a lecture on Stacks and Queues: Slides   "
},
{
  "id": "sec-QueueTerm",
  "level": "1",
  "url": "sec-QueueTerm.html",
  "type": "Section",
  "number": "7.1",
  "title": "Queue Terminology",
  "body": " Queue Terminology  Similar to the stack, the queue is a list-like structure that imposes certain restrictions on accessing its elements. In a queue, elements can only be inserted at the back (enqueued) and removed from the front (dequeued). Queues operate in a manner similar to standing in line at a movie theater ticket counter. If everyone follows the rules, newcomers join the back of the line, and the person at the front of the line is the next to be served. As a result, queues release their elements in the order of their arrival.  In Britain, a line of people is referred to as a \"queue,\" and the act of joining the line to wait for service is known as \"queuing up.\" Accountants have been using queues long before computers existed and refer to them as \"FIFO\" lists, which stands for \"First-In, First-Out.\"  "
},
{
  "id": "sec-ArrayvsQueues",
  "level": "1",
  "url": "sec-ArrayvsQueues.html",
  "type": "Section",
  "number": "7.2",
  "title": "Array-Based vs Linked Queues",
  "body": " Array-Based vs Linked Queues  Both the array-based and linked implementations of queues offer constant-time member functions. The considerations regarding space usage are similar to their respective stack implementations. However, unlike the array-based stack implementation, it is not straightforward to store two queues in the same array unless items are consistently transferred directly from one queue to the other.  The linked queue implementation is a straightforward adaptation of the linked list. In terms of time efficiency, both implementations provide constant-time operations, ensuring efficient queue functionality. However, the choice between array-based and linked implementations may depend on specific space requirements and the need for dynamic resizing.  "
},
{
  "id": "sec-QueuesTutorial",
  "level": "1",
  "url": "sec-QueuesTutorial.html",
  "type": "Section",
  "number": "7.3",
  "title": "Snake with Queues Tutorial",
  "body": " Snake with Queues Tutorial   Goals    The classic game of Snake. Move a \"snake\" (line of sprites) along a 2D grid attempting to run over a randomly placed object to help it grow.    If it runs into itself the game is over and the player has lost.    The object of the game is to make the snake as big as possible.    Represent the snake as a queue    The Snake must avoid bombs that make it smaller      Programming Tasks    Create a function that generates the apple object into a random cell on the grid.    Create a function that generates the bomb object into a random cell on the grid.    Create a function that handles the condition of the snakes head interacting with the food object. Make sure the snakes body grows and the object is gone after a collision. The growing will use enqueuing of a linked list.    Create a function that handles the condition of the snakes head interacting with the bomb object. Make sure the snakes body shrinks and the object is gone after a collision. The shrinking will use dequeuing of a linked list.    Implement enqueue and dequeue functionality      Getting Started  You can begin by downloading the starting class here: SnakeScaffold-1.java   There will be 7 functions that will need editing:    initialize() \/\/Init the queue and size of snake (10 points)    plantBomb() \/\/Choose bomb position on board (10 points)    plantApple() \/\/Choose apple position on board (10 points)    detectApple() \/\/Detect if snake collides with apple (5 points)    detectBomb() \/\/Detect if snake collides with bomb (5 points)    enqueue() \/\/ enqueue a Block object onto the snake (10 points)    dequeue() \/\/dequeue a Block object from the snake (10 points)    Each of these functions have corresponding TO-DO comments on how to complete them. It is recommended to complete these functions in the order above. To move the snake you can use the arrow keys.    Visualizing the Program  You must use the https:\/\/bridges-games.herokuapp.com site to run and test your project. You should be able to use the same BRIDGES account on this site. Once you run your code, the program will continue to run until stopped manually. Once you run your code you can go onto the site, and go to your assignments page. From here you can click the \"connect to game button\" that will start the game.  If you would like to stop your game, you can manually stop the program from your IDE. Before running the program again, you must manually stop your program from the IDE. If you forget this step, the program will run multiple instances of the game creating some errors in server communication. If this happens, just close and reopen the project IDE.   "
},
{
  "id": "sec-IntroSort",
  "level": "1",
  "url": "sec-IntroSort.html",
  "type": "Section",
  "number": "8.1",
  "title": "Introduction to Sorting",
  "body": " Introduction to Sorting  Sorting is a common task in our daily lives, whether it's organizing playing cards, arranging paperwork, or categorizing items like jars of spices. Sorting also plays a crucial role in computing tasks, such as efficiently searching a database, optimizing mailing lists, or assisting algorithms in solving complex problems. For instance, graph algorithms like Kruskal's algorithm require sorting edges by their lengths before processing them.  Due to its significance, sorting has been extensively studied, leading to the development of various algorithms. Some of these algorithms mirror our intuitive sorting strategies, like Insertion Sort, which resembles how we sort cards in a hand of Bridge. Others, designed for sorting large datasets on computers, may seem unfamiliar in their approach. For instance, Quicksort is commonly used in software libraries despite its unconventional method for organizing bills by date.  Sorting remains an active field of research, with ongoing efforts to refine existing algorithms and develop new ones tailored for specific applications. Exploring sorting algorithms not only introduces us to this fundamental problem in computer science but also delves into algorithm design and analysis. We learn different techniques, such as divide and conquer, demonstrated by various sorting approaches. Additionally, sorting algorithms offer valuable insights into algorithm analysis, showcasing variations in growth rates between average and worst cases, exploiting best-case behaviors of other algorithms, and leveraging special-case behaviors for niche applications. Sorting also provides an opportunity to explore lower bound analysis, particularly in the context of external sorting, which deals with sorting large files stored on disk.  "
},
{
  "id": "sec-SortingTerm",
  "level": "1",
  "url": "sec-SortingTerm.html",
  "type": "Section",
  "number": "8.2",
  "title": "Sorting Terminology",
  "body": " Sorting Terminology  The Sorting Problem, as defined, allows input with multiple records having the same key value. However, certain applications require input without duplicate key values. In most cases, sorting algorithms can handle duplicate key values unless explicitly specified otherwise. When duplicates are allowed, there might be an implicit ordering among them based on their occurrence in the input. Preserving this initial ordering among duplicates can be desirable. A sorting algorithm is considered stable if it maintains the relative ordering of records with identical key values. Many of the sorting algorithms discussed in this chapter are stable or can be modified to achieve stability with minor adjustments.  When comparing sorting algorithms, a straightforward approach is to implement both algorithms and measure their running times. This empirical comparison method, however, can be challenging to execute fairly, as the running time of sorting algorithms often depends on specific characteristics of the input values. Factors such as the number of records, key and record sizes, range of key values, and the degree of disorder in the input can significantly impact the relative running times of sorting algorithms.  Traditionally, the analysis of sorting algorithms focuses on measuring the cost in terms of the number of comparisons made between keys. This measure is closely related to the actual running time and offers the advantage of being independent of the specific machine or data type used. However, in certain cases, if the records are large, the physical movement of records during sorting can contribute significantly to the overall running time. In such situations, it may be appropriate to measure the cost by counting the number of swap operations performed by the algorithm. In most applications, it is assumed that all records and keys have a fixed length, and that comparisons and swaps have a constant time complexity regardless of the keys involved. However, special situations arise where the rules for comparing sorting algorithms change. For example, when dealing with records or keys of varying lengths (e.g., sorting variable-length strings), not all comparisons can be assumed to have roughly the same cost. These scenarios require specific analysis techniques and often benefit from specialized sorting techniques tailored to the specific requirements.  "
},
{
  "id": "sec-ComparingRecords",
  "level": "1",
  "url": "sec-ComparingRecords.html",
  "type": "Section",
  "number": "8.3",
  "title": "Comparing Records",
  "body": " Comparing Records  When we want to sort or search for items, we need to compare them to determine their order. For simple values like integers or strings, we can use standard comparison operators. However, when dealing with complex data structures like records with multiple values, we need a way to compare and sort them based on a specific field called the key.  Similarly, when searching for records in a database, we typically define the search criteria using a key value rather than matching the entire contents of the record. The key value helps us locate the desired record efficiently.  To implement sorting and searching, the keys need to be comparable. At the very least, we should be able to determine if two keys are equal or not. Ideally, the keys should establish a total order, allowing us to determine their relative positions. This enables more efficient searching algorithms like binary search.  To extract the key value from a record, we can define a method called \".key()\" or use language-specific features like the Comparable interface in Java. However, using a fixed method name may not always be feasible due to naming conflicts or the need to sort or search based on different keys at different times.  An alternative approach is to use a comparator, which is a function or class responsible for extracting the key from a record. The comparator can be passed as a parameter to sorting or searching functions, allowing flexibility in handling different record types or fields. In some languages, such as Java or C++, a comparator class can be used as a parameter for defining classes like Binary Search Trees (BST).  In situations where a key extraction method is not possible, a general solution is to store key-value pairs explicitly in the data structure. For example, records can be stored in an array where each entry contains both the key value and a pointer to the record itself. This approach minimizes duplication of records and allows us to use different keys for different purposes by storing pointers in separate arrays.  Overall, the key value plays a crucial role in sorting and searching, providing a means to compare and organize data efficiently.  "
},
{
  "id": "sec-InsertionSort",
  "level": "1",
  "url": "sec-InsertionSort.html",
  "type": "Section",
  "number": "8.4",
  "title": "Insertion Sort",
  "body": " Insertion Sort  Imagine you have a stack of phone bills from the past two years, and you want to arrange them in order by date. A natural way to tackle this task is to start with the first two bills and put them in the correct order. Then, take the third bill and place it in its proper position relative to the first two bills. You continue this process, taking each bill and adding it to the already sorted pile. This simple approach serves as the foundation for our first sorting algorithm, known as Insertion Sort.  Insertion Sort works by iterating through the list of records. During each iteration, the current record is inserted at the appropriate position within a sorted list consisting of the records already processed.  Although the best case for Insertion Sort is considerably faster than the average and worst cases, the average and worst cases provide more reliable estimates of the typical running time. However, there are scenarios where we can anticipate the input to be already sorted or nearly sorted. In such cases, Insertion Sort can be a suitable choice, especially when the disordering is minor. Even when the input is not perfectly sorted, Insertion Sort's performance is proportional to the number of inversions, making it efficient for \"nearly sorted\" lists. Shellsort and Quicksort are examples of algorithms that leverage Insertion Sort's near-best-case running time.  When counting comparisons or swaps, similar trends emerge. Each iteration of the inner for loop involves both a comparison and a swap, except for the last comparison that fails the loop's test and does not require a swap. Consequently, the number of swaps in the entire sorting operation is n-1 less than the number of comparisons. The best case has zero swaps, while the average and worst cases exhibit a Θ(n^2) complexity.  As we delve into algorithms with superior growth rates, Insertion Sort becomes less favorable for larger arrays. It is not the most efficient sorting algorithm for most situations. However, there are specific scenarios where it excels, such as when the input is already sorted or when working with very small arrays. Other algorithms with better asymptotic growth rates tend to be more complex, leading to larger constant factors in their running time. While they require fewer comparisons for larger arrays, their cost per comparison is higher. Although this observation may seem less beneficial, there are instances where numerous sorting operations are performed on very small arrays.  "
},
{
  "id": "sec-SelectionSort",
  "level": "1",
  "url": "sec-SelectionSort.html",
  "type": "Section",
  "number": "8.5",
  "title": "Selection Sort",
  "body": " Selection Sort  Let's revisit the task of sorting a stack of phone bills for the past year. Another intuitive approach would be to go through the pile and find the bill for January, setting it aside. Then continue searching for the bill for February and place it after January. Repeat this process with the remaining bills, selecting and arranging them in order until you reach the end. This approach serves as the basis for our last Θ(n^2) sorting algorithm, known as Selection Sort.  During each pass of Selection Sort, we \"select\" the i-th largest key in the array and place it at the end. In simpler terms, we start by finding the largest key in the unsorted portion of the list, then the next largest, and so on. Selection Sort stands out with its minimal number of swaps. While searching for the next largest key value requires scanning through the entire unsorted section, only one swap is needed to position the record correctly. Consequently, the total number of swaps required will be n-1. It's worth noting that any algorithm can be implemented in various ways. For instance, we could have designed Selection Sort to find the smallest record, followed by the next smallest, and so forth. However, in our version, we aimed to closely resemble the behavior of our Bubble Sort implementation. This highlights that Selection Sort is essentially a modified version of Bubble Sort, where we keep track of the position of the record to be selected and perform a single swap at the end.  "
},
{
  "id": "sec-SearchSortLecture",
  "level": "1",
  "url": "sec-SearchSortLecture.html",
  "type": "Section",
  "number": "8.6",
  "title": "Searching and Sorting Algorithm Lecture",
  "body": " Searching and Sorting Algorithm Lecture  Here is a link to a lecture on Searching and Sorting Algorithms: Slides   "
},
{
  "id": "sec-BinarySearchLab",
  "level": "1",
  "url": "sec-BinarySearchLab.html",
  "type": "Section",
  "number": "8.7",
  "title": "Binary Search Lab 1",
  "body": " Binary Search Lab 1  For this lab, you will be given 3 questions. each question has a given array, a target value to look for, and a table to fill out the binary search algorithm process. In the table, for each iteration of the algorithm, you will write the: iteration, low, mid, high, comparison, reduce search space, subrange, and adjustment values. You can see examples of this assignment in the lecture slides.  Please download this file to get started: Binary Search Assignment   "
},
{
  "id": "sec-BinaryLab",
  "level": "1",
  "url": "sec-BinaryLab.html",
  "type": "Section",
  "number": "8.8",
  "title": "Binary Search Lab 2",
  "body": " Binary Search Lab 2   Goals  Understand the Binary Search and apply its application to an array.    Tasks  We will be performing a binary search on a BRIDGES array. The array is populated of size 30 and each element holds the value of its index making it a sorted array. Your job is to find the value 3, 25 and value 100 (doesn't exist) using binary search.  Here is the starting file: BridgesArrayBinarySearch.java     Implement the Binary Search algorithm in the 'BinarySearch' function. (10 points)    For each iteration of the algorithm, color the low and high index green, the mid index 'orange', and the found element 'red' if it exists. (5 points)    Visualize the array at every iteration to see how the algorithm progresses. (5 points)    Add the link and images to the visualization that show the results of searching for 3, 25, and 100. (5 points)    To have multiple visualizations, change the integer value parameter on the line the bridges object is created. This assignment uses BRIDGES to visualize the array and the color attributes added. Every time you call the bridges visualize function, an additional visualization is added in a slideshow view. You can then play\/step through each iteration of your algorithm to help learn and debug binary search. The scaffolded code comes with the main function and 1 static function. The static function is the binary search algorithm that you will implement. The other function is just a main function that you will not need to change except for your BRIDGES credentials and the value you are searching for.   "
},
{
  "id": "sec-SortingBenchmark",
  "level": "1",
  "url": "sec-SortingBenchmark.html",
  "type": "Section",
  "number": "8.9",
  "title": "Sorting Benchmark",
  "body": " Sorting Benchmark   Goals  Understand the performance of sorting algorithms by implementing and benchmarking them.    Tasks  We will be sorting arrays of integers. Here is the starting file: sorting.java     Implement selection sort algorithm in pseudocode. (5 points)    Implement bubble sort algorithm in pseudocode. (5 points)    Implement selection sort algorithm in Java (10 points)    Implement bubble sort algorithm in Java (10 points)    Document how the algorithm works in your code. (5 points)    Benchmark the two algorithms against the standard sorting algorithm provided in the standard library.    Take the BRIDGES Survey about the assignment (5 points)    This assignment uses BRIDGES to visualize the runtime of sorting algorithms based on the size of input array. You will need your BRIDGES account with the api key to visualize the assignment.  The scaffolded code comes with the main function and 3 static functions. One of the static functions does the built-in java sorting algorithm. You must write the selection sort algorithm in the 'sort' function and the bubble sort algorithm in the 'bubbleSort' function. The main function creates the BRIDGES objects and sorting benchmark objects for you. The input to these functions is a basic array, so no need to make any BRIDGES calls. Just write the sorting function to sort the input array. BRIDGES will log the time for you.   "
},
{
  "id": "sec-RecIntro",
  "level": "1",
  "url": "sec-RecIntro.html",
  "type": "Section",
  "number": "9.1",
  "title": "Introduction to Recursion",
  "body": " Introduction to Recursion  An algorithm or function is considered recursive when it calls itself to perform a portion of its task. Recursion offers a way to solve complex problems using concise, easily understandable, and efficient programs. It involves breaking down a large problem into one or more sub-problems that have the same structure as the original problem but are simpler to solve. This process continues until the sub-problems become straightforward enough to be solved without further division. The final solution is then obtained by combining the solved components.  To ensure the success of a recursive approach, the recursive call to the function must operate on a smaller problem compared to the original one. Typically, a recursive algorithm consists of two parts:    The base case, which handles a simple input that can be solved directly without further recursion.    The recursive part, which includes one or more recursive calls to the algorithm. In each recursive call, the parameters should be closer to the base case than those in the original call.    Recursion doesn't have a direct counterpart in everyday problem-solving, which can make it challenging to grasp initially. When first learning recursion, it's common to focus on understanding the recursive process. However, when writing recursive functions, it's best to shift the focus away from the inner workings of recursion and instead concentrate on the recursive call. Trust that the sub-problems will be handled correctly and focus on defining the base cases and how to combine the results of the sub-problems.  Recursion is primarily used as a tool to simplify algorithm design and description. It may not always yield the most efficient program since function calls involved in recursion tend to have higher overhead compared to other alternatives like using a while loop. However, recursive approaches generally provide reasonably efficient algorithms. If needed, the clear and recursive solution can be further optimized to achieve faster implementations.  When we solve a \"big\" problem recursively, we break it down into smaller versions of the problem and solve each of them. The solutions to these smaller problems are then used to solve the original \"big\" problem. Recursive problem-solving involves tackling the smaller versions in a similar manner.  For instance, let's consider the task of summing values in an array. What sets apart summing the first 50 elements from summing the first 100 elements? We can use the same approach for both scenarios. In fact, we can even leverage the solution to the smaller problem (summing the first 50 elements) to help us solve the larger problem (summing the first 100 elements).  "
},
{
  "id": "sec-RecurCode",
  "level": "1",
  "url": "sec-RecurCode.html",
  "type": "Section",
  "number": "9.2",
  "title": "Tracing Recursive Code",
  "body": " Tracing Recursive Code  When writing a recursive function, it is best to approach it from a top-down perspective. Instead of worrying about how the recursive call solves the sub-problem, trust that it will provide the correct solution. Treat the result of the recursive call as if it were a reliable library function that helps solve the original problem accurately.  However, when you need to read or trace a recursive function, it is essential to consider how the function works. Tracing a few recursive functions is an excellent way to understand the behavior of recursion. As you become more familiar with tracing, you will find that you rarely need to delve into all the details. You will gain confidence in your understanding of recursion.  Remember that information can be passed from one recursive call to another through function parameters. This happens on increasingly smaller problems until a base case is reached during the winding phase. Then, as the series of recursive calls unwinds, a return value is passed back. It's important not to overlook the unwinding phase, as it completes the recursive process.  "
},
{
  "id": "sec-RecursionLecture",
  "level": "1",
  "url": "sec-RecursionLecture.html",
  "type": "Section",
  "number": "9.3",
  "title": "Recursion Lecture",
  "body": " Recursion Lecture  Here is a link to a lecture on Recursion: Slides   "
},
{
  "id": "sec-QSAPractice",
  "level": "1",
  "url": "sec-QSAPractice.html",
  "type": "Section",
  "number": "9.4",
  "title": "Quicksort Algorithm Practice",
  "body": " Quicksort Algorithm Practice   Goals  Understand the performance of the Lomuto Quicksort Algorithm compared to the past sorting algorithms implemented this semester. Based on your completion of this assignment, you may receive up to 10% extra credit on your previous test.    Tasks  We will be sorting arrays of integers using the Lomuto Quicksort Algorithm. To do this, you will be using the same benchmarking code from your past lab.    Implement the Lomuto Quicksort algorithm in pseudocode. (1 point)    Implement the Lomuto Quicksort algorithm in your past benchmarking Java project (2 points)    Document how the algorithm works in your code using comments. (1 point)    Benchmark the algorithm against the other algorithms implemented in the past lab and show the visualization link. (1 point)    This assignment uses BRIDGES to visualize the runtime of sorting algorithms based on the size of input array. You will need your BRIDGES account with the api key to visualize the assignment.   "
},
{
  "id": "sec-BinaryIntro",
  "level": "1",
  "url": "sec-BinaryIntro.html",
  "type": "Section",
  "number": "10.1",
  "title": "Binary Trees Introduction",
  "body": " Binary Trees Introduction  Tree structures are valuable for managing extensive data collections efficiently. Among various tree structures, binary trees are commonly employed and relatively straightforward to implement. Although binary trees are frequently used for searching purposes, their applications extend beyond that. They can accelerate tasks such as job prioritization, representing mathematical expressions and program syntax, and organizing data for compression algorithms. In this chapter, we will explore the terminology associated with binary trees, methods for traversing trees, techniques for implementing tree nodes, and several examples of binary trees.  "
},
{
  "id": "sec-BTDef",
  "level": "1",
  "url": "sec-BTDef.html",
  "type": "Section",
  "number": "10.2",
  "title": "Binary Trees Definitions and Properties",
  "body": " Binary Trees Definitions and Properties  A binary tree is a collection of nodes, with either no elements (empty tree) or a root node and two disjoint binary subtrees, known as the left and right subtrees. The root's children are the roots of these subtrees, and an edge connects a node to each of its children. The node is considered the parent, and the children are its direct descendants.  A sequence of nodes, n1, n2, ..., nk, where ni is the parent of ni+1 for 1≤i less than k, is called a path from n1 to nk. The length of the path is k-1. If there exists a path from node R to node M, R is referred to as the ancestor of M, and M is the descendant of R. Hence, all nodes in the tree are descendants of the root, and the root is the ancestor of all nodes.  The depth of a node M in the tree is the length of the path from the root to M. The height of a tree corresponds to the depth of its deepest node. Nodes at the same depth are on the same level within the tree. The root is at level 0 and has a depth of 0. A leaf node is a node with no children, while an internal node has at least one non-empty child.  A recursive data structure is a data structure that includes smaller or simpler instances of the same data structure within itself. Linked lists and binary trees are examples of recursive data structures. In a linked list, it can be defined as either an empty list or a node followed by another list. Similarly, a binary tree is commonly defined as an empty tree or a node connected to two binary trees, one serving as its left child and the other as its right child.  "
},
{
  "id": "sec-BTTraversals",
  "level": "1",
  "url": "sec-BTTraversals.html",
  "type": "Section",
  "number": "10.3",
  "title": "Binary Tress Traversals",
  "body": " Binary Tress Traversals   Binary Tress Traversals  Frequently, we have the need to traverse a binary tree and perform a specific action on each node, such as printing its contents. This process of visiting all the nodes in a specific order is known as a traversal. An enumeration of the tree's nodes is a traversal that lists every node exactly once. In certain applications, the order in which nodes are visited may not matter as long as each node is visited only once. However, for other applications, it is crucial to visit the nodes in an order that preserves a specific relationship.    Preorder Traversal  For example, we might wish to make sure that we visit any given node before we visit its children. This is called a preorder traversal.    Postorder Traversal  In some cases, we may want to visit each node in a binary tree only after visiting its children and their subtrees. This is necessary, for example, when we want to free up memory by deleting all nodes in the tree. We need to delete the children of a node before deleting the node itself. However, to do this, we must first delete the children's children, and so on. This type of traversal, where we visit the children nodes first and then the parent node, is known as a postorder traversal.    Inorder Traversal  An inorder traversal first visits the left child (including its entire subtree), then visits the node, and finally visits the right child (including its entire subtree). The binary search tree makes use of this traversal to print all nodes in ascending order of value.    Implementation  Next, we will explore different implementations for tree traversals. However, before we proceed, we need to define an Abstract Data Type (ADT) for binary tree nodes, which we will call BinNode. Similar to how a linked list consists of a collection of link objects, a tree is composed of a collection of node objects. The BinNode ADT provides member functions to set or retrieve the element value, obtain a pointer to the left child, retrieve a pointer to the right child, and determine if the node is a leaf. This class will be utilized in the subsequent binary tree structures that will be discussed.  Adding a parent pointer to a node can provide convenient upward movement within the tree, similar to adding a link to the previous node in a doubly linked list. However, in practice, the use of a parent pointer is often unnecessary and can increase the space overhead for the tree implementation. Moreover, relying heavily on the parent pointer can indicate a misunderstanding of recursion and result in poor programming practices. Before deciding to use a parent pointer, it's essential to consider if there is a more efficient alternative.  In pointer-based node implementations, an important design decision is whether to use the same class definition for both leaf nodes and internal nodes. While using the same class simplifies the implementation, it may not be the most space-efficient approach. Some applications only require data values for the leaf nodes, while others need different types of values for internal and leaf nodes. It may seem wasteful to store child pointers in leaf nodes if they have no children. Thus, having distinct implementations for internal and leaf nodes can provide space optimization benefits.   "
},
{
  "id": "sec-BinarySearchTrees",
  "level": "1",
  "url": "sec-BinarySearchTrees.html",
  "type": "Section",
  "number": "10.4",
  "title": "Binary Search Trees",
  "body": " Binary Search Trees  A binary search tree (BST) is a binary tree that satisfies the binary search tree property. According to this property, all nodes in the left subtree of a node with a key value K have key values less than or equal to K, and all nodes in the right subtree have key values greater than K. An important consequence of this property is that when the BST nodes are printed using an inorder traversal, the resulting enumeration will be in sorted order, from the lowest key value to the highest.  The first operation we will examine is the search operation, which finds the record that matches a given key. In the BST class, the public member function find calls the private member function findhelp. The find method takes the search key as an explicit parameter and the BST as an implicit parameter, and it returns the record that matches the key. However, the search operation is best implemented as a recursive function that takes the root of a subtree and the search key as parameters.  Removing a node from a BST is a bit trickier than inserting a node, but it is not complicated if all of the possible cases are considered individually. Before tackling the general node removal process, we will first see how to remove from a given subtree the node with the largest key value.  "
},
{
  "id": "sec-BTLecture",
  "level": "1",
  "url": "sec-BTLecture.html",
  "type": "Section",
  "number": "10.5",
  "title": "Binary Trees Lecture",
  "body": " Binary Trees Lecture  Here is a link to a lecture on Trees: Slides   Here is a link to a lecture on Binary Tree Traversals: Slides   "
},
{
  "id": "sec-BSTExercise",
  "level": "1",
  "url": "sec-BSTExercise.html",
  "type": "Section",
  "number": "10.6",
  "title": "Binary Search Tree Exercise",
  "body": " Binary Search Tree Exercise   Goals  The purpose of this assignment is to learn to    Access remote data through BRIDGES.    Manipulate a binary search tree using the earthquake magnitude (or some other attribute) as a search key    Traverse BST using different methods.      Programming Tasks  Starting file: BST_eq.java   Build a binary search tree where each node represents an earthquake record  Steps:    Open your base code.    Plug in your credentials.    Complete the insert_R() function to recursively insert objects into the binary search tree. You should insert objects based on the magnitude of the earthquake.    Perform a traversal on the completed BST.    Perform a binary search to find the biggest earthquake based on magnitude and color it red.      Building The BST    Your BSTElement will use EarthquakeUSGS as the generic parameter, using Double as the key type. The key is will be the magnitude of the earthquake.    Write an insert() method, creating and inserting earthquake records into a binary search tree, using the magnitude of the quake as a search key. Color the nodes less than 3.0 magnitude orange and the nodes greater than or equal to 3.0 magnitude green (10 points)    Color the root node in a unique color. (done for you)    Perform a traversal of the tree and color leaf nodes black. (10 points)    Traverse the tree to find the largest quake and style it RED. (5 points)    Once completed, submit your code and link to the visualization. (2 points)     "
},
{
  "id": "sec-HashIntro",
  "level": "1",
  "url": "sec-HashIntro.html",
  "type": "Section",
  "number": "11.1",
  "title": "Introduction",
  "body": " Introduction  Hashing is a method used to store and retrieve records in a database based on a search key value. It enables operations like insertion, deletion, and searching, which can be performed in constant time when properly implemented. A well-tuned hash system typically examines only one or two records for each search, insert, or delete operation, outperforming other methods like binary search on a sorted array or a binary search tree with O(logn) average costs. Despite its simple concept, proper implementation of hashing can be challenging, requiring designers to pay close attention to all the details involved.  In a hash system, records are stored in an array known as a hash table (HT). Hashing involves computing a hash function (denoted as h) on a search key K to identify the position in HT where the record with key K should be placed. As records are stored based on the address calculation, they are not ordered by value. Each position in the hash table is referred to as a slot, with the number of slots denoted by M, starting from 0 to M-1.  The primary goal of a hashing system is to ensure that for any key value K and hash function h, the resulting position i=h(K) in the table is such that 0≤i Less than M, and the key of the record stored at HT[i] is equal to K.  However, hashing may not be suitable for applications that permit multiple records with the same key value or when answering range searches. It cannot easily find all records within a specific range or determine the minimum or maximum key value or visit records in key order. Yet, hashing excels at answering exact-match queries, making it the preferred search method for such applications when implemented correctly. Despite its efficiency, there are various approaches to hashing, and inefficient implementations can be devised. Hashing is suitable for both in-memory and disk-based searching and is one of the most widely used methods for organizing large databases stored on disk, alongside the B-tree.  "
},
{
  "id": "sec-OpenHashing",
  "level": "1",
  "url": "sec-OpenHashing.html",
  "type": "Section",
  "number": "11.2",
  "title": "Open Hashing",
  "body": " Open Hashing  Hash functions aim to minimize collisions, but in practice, some collisions are inevitable. Thus, collision resolution policies are essential in hashing implementations. There are two primary classes of collision resolution techniques: open hashing (or separate chaining) and closed hashing (or open addressing). Despite the confusing naming convention, open hashing involves storing collisions outside the table, while closed hashing stores one of the records in another slot within the table.  In the simplest form of open hashing, each slot in the hash table is the head of a linked list. All records that hash to a particular slot are placed in that slot's linked list. This creates a structure where each slot points to a linked list that holds the records associated with that specific slot. The hash function used in this case is the straightforward mod function. The following figure illustrates a hash table employing open hashing and the mod function for hash calculations.  Open hashing is well-suited for scenarios where the hash table is stored in main memory, and the lists are implemented using standard in-memory linked lists. However, using open hashing to store a hash table on disk efficiently poses challenges. Since members of a linked list may be stored in different disk blocks, searching for a specific key value would require multiple disk accesses, undermining the purpose of hashing.  Open hashing shares some similarities with Binsort. In open hashing, each record is placed in a bin, even though multiple records may hash to the same bin. This initial binning significantly reduces the number of records accessed during a search operation. Similarly, a simple Binsort organizes records into bins, reducing the number of records in each bin to a small manageable number that can be sorted using a different approach.  "
},
{
  "id": "sec-BucketHash",
  "level": "1",
  "url": "sec-BucketHash.html",
  "type": "Section",
  "number": "11.3",
  "title": "Bucket Hashing",
  "body": " Bucket Hashing  Closed hashing stores all records directly in the hash table. Each record with a key value kR has a home position at h(kR), which is computed by the hash function. If the home position is occupied during insertion, the record is stored in another slot within the table, and the collision resolution policy determines which slot that will be. This policy is also followed during search to retrieve any record not found in its home position, repeating the collision resolution process.  One implementation of closed hashing involves grouping hash table slots into buckets. The M slots of the hash table are divided into B buckets, each containing M\/B slots. Records are assigned to the first available slot within a bucket based on the hash function. If a slot is already taken, the bucket slots are searched sequentially until an open slot is found. If a bucket becomes full, records are stored in an overflow bucket with infinite capacity located at the end of the table. All buckets share the same overflow bucket. A well-designed implementation uses a hash function that evenly distributes records among the buckets to minimize the use of the overflow bucket.  During a search for a record, the key is hashed to determine the bucket that should contain the record. The records in this bucket are then searched. If the desired key value is not found and the bucket still has free slots, the search is complete. However, if the bucket is full, it is possible that the desired record is stored in the overflow bucket.  "
},
{
  "id": "sec-CollisionRes",
  "level": "1",
  "url": "sec-CollisionRes.html",
  "type": "Section",
  "number": "11.4",
  "title": "Collision Resolution",
  "body": " Collision Resolution  Now, let's focus on the most commonly used form of hashing: closed hashing without bucketing, which employs a collision resolution policy that can potentially use any slot in the hash table. When inserting a record, collision resolution aims to find a free slot in the hash table if the record's home position is already taken. We can think of collision resolution methods as generating a sequence of hash table slots that may accommodate the record. The first slot in the sequence is the home position for the key. If it's occupied, the collision resolution policy moves to the next slot in the sequence. If that slot is also taken, it searches for another, and so on. This sequence of slots is called the probe sequence, generated by a probe function we'll call p.  "
},
{
  "id": "sec-HashLecture",
  "level": "1",
  "url": "sec-HashLecture.html",
  "type": "Section",
  "number": "11.5",
  "title": "Hash Tables and Hash Maps Lecture",
  "body": " Hash Tables and Hash Maps Lecture  Here is a link to download the lecture on Hash Tables and Hash Maps: Slides   "
},
{
  "id": "sec-HashMapLab",
  "level": "1",
  "url": "sec-HashMapLab.html",
  "type": "Section",
  "number": "11.6",
  "title": "HashMap Lab",
  "body": " HashMap Lab   Counting word appearances using a Dictionary and a custom implementation  A Dictionary (sometimes called associative arrays) enable to store and retrieve (key, value) pairs. In this assignment, they will be useful to count how many times a particular word appears in Shakespeare's work. The keys are going to be words. And the value associated with that key is going to be how many times that word appears. You will be building the hashmap yourself given the scaffolded code. The only file you will need to edit is the HashTabel.java file.    Getting Started    Get your IDE ready for a BRIDGES project and download the scaffold code: hashassignment.zip     Open up the HashTable.java file.    Complete all the todo functions    Plug in your credentials in PQBook.java.    Run the program to see the visual of the hashtable      Tasks to Complete    Complete the get() function in HashTable.java | 10 pts    Complete the set() function in HashTable.java | 10pts    Complete the delete() function in HashTable.java | 10pts    Complete the resize() function in HashTable.java | 10pts      Functionality Information  The hashtable uses chaining for collisions so each index in the hashtable will hold a linked list of nodes. Each Node is an object that holds the key, value, and next position in the linked list.  The get() function is used to get a value from the hashtable using the key passed in as a parameter. You will get the index into the hashtable from the hash function and loop over the list at that index to get the object.  The set() function is used to set an object with the given key to a given value. If an object with the key is in the hashmap, you can set the object's value. If not, add a new object with the key-value pair.  The delete() function removes an object from the hashtable given the key.  The resize() function resizes the hashtable once a loadFactor has been reached. This condition is calculated for you in the set() function. Once you set a new object in the hashtable, it checks if that loadFactor is reached. This relies on the count variable being accurate so update the count variable every time a new objects is added. If so, resize is called. In the resize function, you can interate over the hashtable by doing: for(Node node : this.table). This will get the first node at each index in the hashtable.   "
},
{
  "id": "sec-GraphsIntro",
  "level": "1",
  "url": "sec-GraphsIntro.html",
  "type": "Section",
  "number": "12.1",
  "title": "Graphs Chapter Introduction",
  "body": " Graphs Chapter Introduction  Graphs offer unparalleled versatility as a data structure. They consist of nodes and edges, where edges connect pairs of nodes. Notably, both trees and lists can be considered as specific cases of graphs.  Graphs find extensive use in modeling real-world systems and abstract problems, making them a preferred data structure in numerous applications. Below are some examples of the diverse problems for which graphs are routinely employed:    Representing connectivity in computer and communication networks.    Abstractly mapping locations and distances, useful for calculating shortest routes in GPS navigation.    Modeling flow capacities in transportation networks to identify bottlenecks.    Solving pathfinding problems from a starting condition to a goal condition, commonly in AI and gaming applications.    Depicting computer algorithms, showcasing transitions between program states.    Determining an optimal order for completing subtasks in complex activities, like constructing large buildings.    Modeling relationships, such as family trees, organizational structures, and scientific classifications.    "
},
{
  "id": "sec-GraphTrav",
  "level": "1",
  "url": "sec-GraphTrav.html",
  "type": "Section",
  "number": "12.2",
  "title": "Graph Traversals",
  "body": " Graph Traversals  Graph traversal, similar to tree traversal, involves visiting vertices in a specific order based on the graph's topology. Just as tree traversals have various orders like preorder, inorder, and postorder, graph traversals also have different orders, each suited for specific applications. For example, in artificial intelligence programming, graph traversal is crucial to solving problems involving a collection of states connected by edges.  Graph traversal algorithms start from a designated start vertex and aim to visit all remaining vertices. These algorithms face challenges, such as dealing with disconnected graphs where not all vertices are reachable from the start vertex, or handling cycles without getting stuck in infinite loops.  To address these issues, graph traversal algorithms utilize a \"VISITED\" flag to keep track of visited vertices. At the beginning of the traversal, no vertices have the \"VISITED\" flag set. As the algorithm visits a vertex, it marks it as \"VISITED\" to avoid revisiting it. This mechanism prevents infinite loops when encountering cycles.  Once the traversal completes, we can check if all vertices have the \"VISITED\" flag set to ensure that all vertices have been processed. If some vertices remain unflagged, we can continue the traversal from another unvisited vertex. Remarkably, this process applies to both directed and undirected graphs.  "
},
{
  "id": "sec-GraphsLeture",
  "level": "1",
  "url": "sec-GraphsLeture.html",
  "type": "Section",
  "number": "12.3",
  "title": "Graphs Leture",
  "body": " Graphs Leture  Here is a link to download the lecture on graphs: Slides   "
},
{
  "id": "sec-EarthquakeAssignment",
  "level": "1",
  "url": "sec-EarthquakeAssignment.html",
  "type": "Section",
  "number": "12.4",
  "title": "Earthquake Tracker Assignment",
  "body": " Earthquake Tracker Assignment   Goals  The purpose of this assignment is to learn to    Access and manipulate remote data through BRIDGES.    Manipulate a GraphAdjList object.    Display a location on a map.    Optionally, perform basic graph operation (connected component)    Please download: Earthquake.zip     Programming Tasks  Grab recent earthquake data and build a graph representing the locations of the 100 strongest earthquakes.    Open your scaffolded code.    Plug in your credentials.    Get the most recent 10,000 earthquakes.    Only retain the 100 highest magnitude earthquakes.      Place Earthquakes on the map    Create a graph where each earthquake is a vertex.    Add no edges for now.    Pin earthquakes at their longitude and latitude.    Tweak the appearance of vertices if you want (e.g., use a different symbols for earthquake in Hawaii or Alaska).    Compile, run, and visualize.      Build a graph based on distances  For each pair of earthquakes:    Compute the distance using calcDistance.    If the earthquakes are closer than 500km, add an edge between them.    Compile, run, and visualize.    Show just the graph    Deactivate the map overlay (already done in the scaffolding).    Unpin the vertices by setting their location to infinity.    Compile, run, and visualize.     "
},
{
  "id": "colophon-2",
  "level": "1",
  "url": "colophon-2.html",
  "type": "Colophon",
  "number": "",
  "title": "Colophon",
  "body": " This book was authored in PreTeXt .  "
}
]

var ptx_lunr_idx = lunr(function () {
  this.ref('id')
  this.field('title')
  this.field('body')

  ptx_lunr_docs.forEach(function (doc) {
    this.add(doc)
  }, this)
})
