var ptx_lunr_search_style = "textbook";
var ptx_lunr_docs = [
{
  "id": "colophon-1",
  "level": "1",
  "url": "colophon-1.html",
  "type": "Colophon",
  "number": "",
  "title": "Colophon",
  "body": "   example.org   https:\/\/example.org   copyright  "
},
{
  "id": "section-Bridges",
  "level": "1",
  "url": "section-Bridges.html",
  "type": "Section",
  "number": "1.1",
  "title": "About BRIDGES",
  "body": " About BRIDGES  BRIDGES is an API\/Toolkit for providing easy-to-use interfaces to real-world data and internet-based information systems, that are exciting and engaging, such as social network data (Reddit), entertainment (movies, songs), encyclopaedia type systems (Wikipedia, Gutenberg books), scientific\/engineering (USGIS Earthquakes) or geographic (city, county, countries, OpenStreet Map, Elevation), entertainment repositories (IMDB, GeniusAPI) data. The BRIDGES toolkit provides a set of classes in C++, Java and Python, to support early CS courses, such CS1\/CS2 and Data Structures\/Algorithm Analysis. See the BRIDGES home page for examples and a short video on an introduction to BRIDGES.  Bridges home page  How does Bridges help:    Provides easy-to-use interfaces to exciting, engaging real-world data (social networks, scientific data, etc), to make it possible for their use in freshmen\/sophomore level CS courses    Makes it easy to visualize course assignments in a CS1, CS2, data structures, or algorithm courses    Is carefully designed to augment the student experience in routine introductory courses in Computer Science    "
},
{
  "id": "sec-Setup",
  "level": "1",
  "url": "sec-Setup.html",
  "type": "Section",
  "number": "1.2",
  "title": "Setting up Bridges",
  "body": " Setting up Bridges  Here is a link to setting up Bridges on your computer in NetBeans: Bridges setup   "
},
{
  "id": "section-program",
  "level": "1",
  "url": "section-program.html",
  "type": "Section",
  "number": "1.3",
  "title": "Hello World program",
  "body": " Hello World program   Simple Java Program  Let's write a simple Java program that prints \"Hello, World!\" to the console:  public class HelloWorld { public static void main(String[] args) { System.out.println(\"Hello, World!\"); } }   "
},
{
  "id": "example-1",
  "level": "2",
  "url": "section-program.html#example-1",
  "type": "Example",
  "number": "1.3.1",
  "title": "Simple Java Program.",
  "body": " Simple Java Program  Let's write a simple Java program that prints \"Hello, World!\" to the console:  public class HelloWorld { public static void main(String[] args) { System.out.println(\"Hello, World!\"); } }  "
},
{
  "id": "sec-SmileyFace",
  "level": "1",
  "url": "sec-SmileyFace.html",
  "type": "Section",
  "number": "1.4",
  "title": "Simple Smiley Face",
  "body": " Simple Smiley Face  Here is a link to a simple smiley face program that you can do once Bridges is setup on your computer: Here   "
},
{
  "id": "section-ITOOP",
  "level": "1",
  "url": "section-ITOOP.html",
  "type": "Section",
  "number": "2.1",
  "title": "Introduction to Object-oriented Programming",
  "body": " Introduction to Object-oriented Programming  Object-oriented programming (OOP) is a programming paradigm based on the concept of objects, which are data structures that contain data, in the form of fields (or attributes) and code, in the form of procedures, (or methods). A distinguishing feature of objects is that an object’s procedures provide access to and modify its fields.  In object-oriented programming, computer programs are designed by making them out of objects that interact with one another. There is significant diversity in object-oriented programming, but most popular languages are class-based, meaning that objects are instances of classes, which typically also determines their type.  Object orientation is an outgrowth of procedural programming. Procedural programming is a programming paradigm, derived from structured programming, based upon the concept of the procedure call. Procedures, also known as routines, subroutines, or methods define the computational steps to be carried out.  Any given procedure might be called at any point during a program’s execution, including by other procedures or itself. Procedural programming is a list or set of instructions telling a computer what to do step by step and how to perform from the first code to the second code. Procedural programming languages include C, Fortran, Pascal, and BASIC.  The focus of procedural programming is to break down a programming task into a collection of variables, data structures, and subroutines, whereas in object-oriented programming it is to break down a programming task into objects that expose behavior (methods) and data (fields) using interfaces. The most important distinction is that while procedural programming uses procedures to operate on data structures, object-oriented programming bundles the two together, so an object, which is an instance of a class, operates on its “own” data structure.  "
},
{
  "id": "sec-POOOP",
  "level": "1",
  "url": "sec-POOOP.html",
  "type": "Section",
  "number": "2.2",
  "title": "Principles of Object-oriented Programming",
  "body": " Principles of Object-oriented Programming   Encapsulation  Encapsulation refers to the creation of self-contained modules (classes) that bind processing functions to its data members. The data within each class is kept private. Each class defines rules for what is publicly visible and what modifications are allowed.    Inheritancr  Classes may be created in hierarchies, and inheritance lets the structure and methods in one class pass down the class hierarchy. By inheriting code, complex behaviors emerge through the reuse of code in a parent class. If a step is added at the bottom of a hierarchy, only the processing and data associated with that unique step must be added. Everything else above that step may be inherited. Reuse is considered a major advantage of object orientation.    Polymorphism  Object oriented programming lets programmers create procedures for objects whose exact type is not known until runtime. For example, a screen cursor may change its shape from an arrow to a line depending on the program mode. The routine to move the cursor on screen in response to mouse movement can be written for “cursor”, and polymorphism lets the right version for the given shape be called.    Abstraction  An abstraction denotes the essential characteristics of an object that distinguish it from all other kinds of objects and thus provide crisply defined conceptual boundaries, relative to the perspective of the viewer. [Booch] Abstraction denotes a model, a view, or some other focused representation for an actual item. It’s the development of a software object to represent an object we can find in the real world. Encapsulation hides the details of that implementation.   "
},
{
  "id": "sec-UML",
  "level": "1",
  "url": "sec-UML.html",
  "type": "Section",
  "number": "2.3",
  "title": "The Unified Modeling Language",
  "body": " The Unified Modeling Language  The Unified Modeling Language, or UML, is an industry standard graphical notation for describing and analysing software designs. The symbols and graphs used in the UML are an outgrowth of efforts in the 1980’s and early 1990’s to devise standards for Computer-Aided Software Engineering (CASE). UML represents a unification of these efforts. In 1994 - 1995 several leaders in the development of modeling languages, Grady Booch, Ivar Jacobson, and James Rumbaugh, attempted to unify their work. To eliminate the method fragmentation that they concluded was impeding commercial adoption of modeling tools, they developed UML, which provided a level playing field for all tool vendors.  UML has been accepted as a standard by the Object Management Group (OMG). The OMG is a non-profit organization with about 700 members that sets standards for distributed object-oriented computing.  UML was initially largely funded by the employer of Booch, Jacobson and Rumbaugh, aka the three amigos, Rational Software, which was sold to IBM in 2002.  A software model is any textual or graphic representation of an aspect of a software system. This could include requirements, behavior, states or how the system is installed. The model is not the actual system, rather it describes different aspects of the system to be developed. UML defines a set of diagrams and corresponding rules that can be used to model a system. The diagrams in the UML are generally divided into two broad categories or views, static and dynamic.  This course does not provide anywhere near a comprehensive review of the UML. The intent is to introduce you to the basics you need to understand the designs presented in this course. Since there is an excellent chance you will encounter the UML or something very similar to it in your professional career and the diagrams used in this course are used not only in the UML, but in other modeling systems as well.  "
},
{
  "id": "sec-Testing",
  "level": "1",
  "url": "sec-Testing.html",
  "type": "Section",
  "number": "2.4",
  "title": "Testing vs Debugging",
  "body": " Testing vs Debugging  When we “write a program”, we actually spend most of our time testing and debugging. These are two separate things. Testing refers to determining whether the program operates as we intend. Debugging refers to correcting the program once we determine that it is not operating as we intend. So we can only debug to the extent that we have tested and determined that there is a problem that needs to be corrected. Debugging to fix a known problem can sometimes be extremely hard, but is often somewhat mechanical. Testing requires a lot of skill and empathy, in order to think of all of the ways that a program might go wrong (in particular, all of the input paths to the program that might affect its behavior).  "
},
{
  "id": "sec-UnitTest",
  "level": "1",
  "url": "sec-UnitTest.html",
  "type": "Section",
  "number": "2.5",
  "title": "Unit Testing",
  "body": " Unit Testing   Why do we need unit testing?  Software evolves over time. Thus, we should adapt to change rather than stick to a strict plan. Software development is a collaborative process: many individuals work on different parts of the software to build something that meets customer needs, and different individuals perform different roles iteratively to determine a product's future. We work with new technologies, ever-changing requirements, people movement, or a combination of these. All of these determine that software projects involve a lot of uncertainties. People say that the only constant is uncertainty. To overcome the fear and manage these uncertainties, we need a software development practice that can help us produce working software. It must keep things simple and provide us quick feedback in case things go wrong. We need to verify the working software when change happens.  Test-driven development (TDD) is one of the practices of Agile software development that a lot of developers use in some shape or form. The premise of TDD is that you write a failing test case before you write the production code itself. TDD, if done correctly, can help you write software that meets customer expectations, has a simple design, and has fewer defects.    What is Unit Testing?  The main idea of unit testing is testing software with a small piece of source code (unit, component, and\/or function) of the same software. \"Unit testing\" means that the software consists of \"units\" which are separate testable parts of the product. An individual program, class, method, function etc. can be such \"unit\". Unit testing allows checking whether a unit behaves as the developer intended and whether a unit corresponds to the design specifications. Unit testing provides an ability of independent testing for each software unit.  The advantages of unit testing is as follows:    Developers looking to learn what functionality is provided by a unit and how to use it can look at the unit tests to gain a basic understanding of the unit API.    Unit testing allows the programmer to refactor code at a later date, and make sure the module still works correctly (i.e. Regression testing).    The procedure is to write test cases for all functions and methods so that whenever a change causes a fault, it can be quickly identified and fixed. Due to the modular nature of the unit testing, we can test parts of the project without waiting for others to be completed.      What is JUnit?  JUnit, developed by Kent Beck and Erich Gamma, is one of the most popular unit-testing frameworks for Java developers. It was originally based on SUnit, a unit-testing framework written in Smalltalk (developed by Kent Beck). The first version of JUnit was released in 1997.  Prior to the introduction of JUnit, testing discipline was dominated by capture and replay testing tools. These tools were black-box testing tools, which used to capture the state of the system with a given input and then try to replay it. The tests written in such a framework involved an enormous amount of effort. These tools were not designed to unit test a component as they tested the application using its graphical user interface (GUI).  JUnit rejected the idea of GUI-based tests. It instead provided a lightweight framework, which enabled test creation by writing code in Java. This allowed developers to build test suites for every piece of their code. Due to its benefits, JUnit was integrated with all kinds of build tools and integrated development environments (IDEs).  JUnit is a unit testing framework for Java programming language. It plays a crucial role test-driven development, and is a family of unit testing frameworks collectively known as xUnit. JUnit promotes the idea of \"first testing then coding\", which emphasizes on setting up the test data for a piece of code that can be tested first and then implemented. This approach is like \"test a little, code a little, test a little, code a little.\" It increases the productivity of the programmer and the stability of program code, which in turn reduces the stress on the programmer and the time spent on debugging.   "
},
{
  "id": "sec-JUintExample",
  "level": "1",
  "url": "sec-JUintExample.html",
  "type": "Section",
  "number": "2.6",
  "title": "JUint Example",
  "body": " JUint Example   Let us assume that we have a single class, Calculator, in a class library project. Add looks pretty reliable at first glance, but so does all the code you write. You still need to test it, if only to prove your rightness to others.   public class Calculator{ public int Add(int x, int y){ return x + y; } }  public class Calculator{  public int Add(int x, int y){  return x + y;  }  }   Traditionally, we write a console class or main method inside the class, for example,   class CalculatorTest { static void Main(string[] args){ Calculator calculator = new Calculator(); int result = calculator.Add(5, 6); if (result != 11) throw new InvalidOperationException(); } }  class CalculatorTest {  static void Main(string[] args){  Calculator calculator = new Calculator();  int result = calculator.Add(5, 6);  if (result != 11)  throw new InvalidOperationException();  }  }   By using Junit package, we could have:   class CalculatorTest{ public void testAdd(){ Calculator calculator = new Calculator(); assertTrue(11 == calculator.Add(5, 6)); } }  class CalculatorTest{  public void testAdd(){  Calculator calculator = new Calculator();  assertTrue(11 == calculator.Add(5, 6));  }  }   Note:    The key part here is writing a failing test first.    Remember that it is our objective to work in small steps to make sure we will never experience a sudden and unexpected test success or test failure. In practice, most developers take increasingly bigger steps, with the result of frequent surprises when they run their tests. That's where we will hopefully distinguish ourselves as test-first masters by making giant steps when moving in well-known territory and making very tiny forward and sideward steps in unknown terrain and on slippery ground. However, this strategy requires that we know how to make the tiny steps in the first place.    At the beginning of each step, there was a test which was directly or indirectly motivated by the requirements specification. To be able to write this test, we had to make decisions about the public interface desired for our OUT (object under test). This public interface served both to “stimulate” the test object and to verify the correct behavior.    This approach drove and controlled the development of our production code from the tests.    So far, our tests have concentrated exclusively on the externally visible behavior of the OUT. In Java this theoretically includes all methods (and variables) with public, protected, or package scope visibility. In practice we restrict ourselves to use only what is intended to be used from the outside, that is from client code, which usually leaves out protected methods and members available for subclassing.     "
},
{
  "id": "sec-GameTutorial",
  "level": "1",
  "url": "sec-GameTutorial.html",
  "type": "Section",
  "number": "2.7",
  "title": "Bridges Game API Tutorial",
  "body": " Bridges Game API Tutorial  Here is a link to starting out Bridges Game API: Here. It is a simple tutorial to get you started on working with Bridges Game API.  "
},
{
  "id": "sec-DSAlgor",
  "level": "1",
  "url": "sec-DSAlgor.html",
  "type": "Section",
  "number": "3.1",
  "title": "Data Structures and Algorithms",
  "body": " Data Structures and Algorithms   Introduction  To determine the number of cities with a population exceeding 250,000 within a 500-mile radius of Dallas, Texas; to identify the count of employees in my company earning over $100,000 per year; or to assess the feasibility of connecting all telephone customers with less than 1,000 miles of cable, it is insufficient to possess the necessary information alone. We must organize that information in a manner that enables us to promptly find the answers to meet our requirements.  Information representation lies at the core of computer science. Most computer programs are designed not primarily for performing calculations, but for storing and retrieving information—usually with optimal speed. Hence, the study of data structures and the algorithms that manipulate them forms the essence of computer science. This book aims to assist you in comprehending how to structure information to support efficient processing.  In any course on Data Structures and Algorithms, you will typically encounter three key aspects:    Introduction to commonly used data structures and algorithms, which constitute a programmer's fundamental toolkit. For many problems, a data structure or algorithm from this toolkit will provide a suitable solution. Our focus is on data structures and algorithms that have stood the test of time and proven to be most beneficial.    Exploration of tradeoffs and reinforcement of the idea that every data structure or algorithm entails costs and benefits. This is achieved by describing the space and time requirements associated with typical operations for each data structure. Similarly, we examine the time needed for various input types in each algorithm.    Instruction on measuring the effectiveness of a data structure or algorithm. Only through such evaluation can we determine which data structure from our toolkit is most suitable for a new problem. The techniques presented also enable us to assess the merits of new data structures that we or others might invent.    When approaching problem-solving, there are often multiple approaches available. How do we choose among them? At the core of computer program design lie two goals, which can sometimes conflict:    Designing an algorithm that is easy to understand, code, and debug.    Designing an algorithm that efficiently utilizes the computer's resources.    Ideally, a resulting program embodies both of these goals. We could consider such a program to be \"elegant.\" While the algorithms and code examples presented here strive for elegance in this sense, it is not the primary objective of this book to explicitly address issues related to goal. Such concerns primarily fall within the domain of Software Engineering. Instead, our focus primarily revolves around issues related to goal.  How do we measure efficiency? The method employed to evaluate the efficiency of an algorithm or computer program is known as asymptotic analysis. This analysis also allows us to define the inherent complexity of a problem. Throughout the book, we employ asymptotic analysis techniques to estimate the time cost for each algorithm presented. This enables you to compare the efficiency of different algorithms used to solve the same problem and understand how they fare against one another.    Philosophy of Data Structures  You might assume that as computers become more powerful, the importance of program efficiency diminishes. After all, processor speed and memory capacity continue to improve. Can't we rely on tomorrow's hardware to solve today's efficiency issues?  However, our history of computer development has shown that as computing power increases, we tend to tackle increasingly complex problems. This can take the form of more sophisticated user interfaces, larger problem sizes, or previously unsolvable computational challenges. With more complex problems, the demand for computation grows, amplifying the need for efficient programs. Unfortunately, as tasks become more intricate, they often deviate from our everyday experiences. Therefore, today's computer scientists must be well-versed in the principles of efficient program design because their everyday intuitions may not directly apply in the realm of computer programming.  In a broad sense, a data structure encompasses any representation of data and the operations associated with it. Even a simple integer or floating-point number stored in a computer can be seen as a basic data structure. However, the term \"data structure\" commonly refers to the organization or structuring of a collection of data items. For instance, a sorted list of integers stored in an array exemplifies such a structuring. These concepts are further explored in the discussion of Abstract Data Types.  With sufficient space to store a collection of data items, it is always possible to search for specific items, process the items in any desired order, or modify the value of individual items. The most straightforward example is an unsorted array containing all the data items. It is possible to perform all necessary operations on such an unsorted array. However, employing the appropriate data structure can make a significant difference in program efficiency. For instance, searching for a specific record in a hash table is much faster than searching for it in an unsorted array.  An efficient solution is one that solves the problem while adhering to the required resource constraints. These constraints can include the available space to store data (which may involve separate constraints for main memory and disk space) and the allotted time for each subtask. Sometimes, a solution is deemed efficient if it utilizes fewer resources than known alternatives, regardless of meeting specific requirements. The cost of a solution refers to the amount of resources it consumes. Typically, cost is measured in terms of a key resource, such as time, assuming that the solution fulfills other resource constraints as well.    Selecting a Data Structure  It is an essential reminder that people develop programs to solve problems, yet programmers sometimes overlook this fact. Therefore, it is crucial to always bear this truth in mind when choosing a data structure to tackle a specific problem. The first step towards selecting the right data structure is analyzing the problem and determining the performance objectives that must be met. Without this initial analysis, program designers often make the mistake of using a familiar but inappropriate data structure, resulting in a slow program. Conversely, there is no point in adopting a complex representation to \"enhance\" a program that can achieve its performance goals through a simpler design.  To select a data structure for problem-solving, follow these steps:    Analyze the problem and identify the essential operations that the data structure must support. These operations may include inserting a data item into the structure, deleting a data item, and finding a specific data item.    Quantify the resource limitations for each operation.    Choose the data structure that best satisfies the requirements identified in the previous steps.    This three-step approach to data structure selection operationalizes a data-centric perspective in the design process. The primary focus is on the data and the operations performed on them. The next concern is finding the appropriate representation for the data, followed by implementing that representation.  Resource constraints, especially for critical operations like searching, inserting, and deleting data records, typically guide the selection of a data structure. Addressing questions regarding the relative significance of these operations can help in making informed decisions. Whenever you face the task of choosing a data structure, consider asking yourself the following three questions:    Are all data items inserted into the structure at the beginning, or are insertions interspersed with other operations? Static applications, where data is loaded initially and remains unchanged, often benefit from simpler data structures for efficient implementation. In contrast, dynamic applications frequently require more intricate structures.    Is it possible to delete data items? If so, this may introduce additional complexity into the implementation.    Are all data items processed in a defined order, or does the search involve locating specific data items? \"Random access\" searches typically necessitate more complex data structures.    By carefully considering these factors and evaluating the requirements of your problem, you can make informed choices when selecting an appropriate data structure.   "
},
{
  "id": "sec-AbstractDT",
  "level": "1",
  "url": "sec-AbstractDT.html",
  "type": "Section",
  "number": "3.2",
  "title": "Abstract Data Types",
  "body": " Abstract Data Types   Abstract Data Types  This module introduces terminology and definitions related to techniques used in managing the complexity of computer programs. It provides clear explanations for the fundamental yet sometimes elusive terms \"data item\" and \"data structure.\" We begin by discussing the basic building blocks on which data structures are constructed.  A type represents a set of values. For instance, the Boolean type consists of the values true and false, while the integer type encompasses whole numbers. An integer is considered a simple type since its values have no internal components. On the other hand, a bank account record typically comprises various pieces of information like name, address, account number, and balance. Such a record exemplifies an aggregate type or composite type. A data item refers to a piece of information or a record whose value is drawn from a particular type. We say that a data item belongs to a type.  A data type is a type that comes with a collection of operations used to manipulate it. For example, an integer variable belongs to the integer data type, and addition is one operation applicable to the integer data type.  It is important to distinguish between the logical concept of a data type and its physical implementation in a computer program. For instance, there exist two traditional implementations for the list data type: the linked list and the array-based list. This means that a list data type can be implemented using either a linked list or an array. However, when we are working with a more complex design and need to utilize a list, we do not necessarily require knowledge of how it is implemented. For example, a list might be used to aid in implementing a graph data structure.  Similarly, the term \"array\" can refer to both a data type and an implementation. In computer programming, \"array\" commonly denotes a contiguous block of memory locations, where each location stores a fixed-length data item. In this sense, an array is a physical data structure. However, \"array\" can also denote a logical data type consisting of a collection of data items, typically of the same type, each identified by an index number. Arrays can be implemented in various ways beyond a contiguous block of memory locations. For instance, a sparse matrix is a large two-dimensional array that only stores a relatively small number of non-zero values. It can be implemented using a linked structure or a hash table, while still providing the same interface as if it were implemented as a block of contiguous memory locations using traditional row and column indices.  An abstract data type (ADT) specifies a data type within a particular programming language, independent of any specific implementation. The ADT interface is defined in terms of a type and a set of operations associated with that type. The behavior of each operation is determined by its inputs and outputs. An ADT does not dictate how the data type should be implemented. The implementation details are hidden from the ADT's user, ensuring encapsulation and protecting them from external access.  A data structure represents the concrete implementation of an ADT. In object-oriented languages, an ADT and its implementation together form a class. Each operation associated with the ADT is implemented as a member function or method. The variables that define the storage space required by a data item are known as data members. An object is an instance of a class, meaning it is created and occupies memory during program execution.  The term \"data structure\" often refers to data stored in a computer's main memory, while \"file structure\" typically describes the organization of data on peripheral storage devices like disk drives or CDs.   "
},
{
  "id": "sec-ADTGenerics",
  "level": "1",
  "url": "sec-ADTGenerics.html",
  "type": "Section",
  "number": "3.3",
  "title": "Abstract Data Types and Generics",
  "body": " Abstract Data Types and Generics   Algorithms  At the heart of computer program design are two (sometimes conflicting) goals:    To design an algorithm that is easy to understand, code, and debug.    To design an algorithm that makes efficient use of the computer's resources.    The method for evaluating the efficiency of an algorithm or computer program is called asymptotic analysis.      Guideline of Data Structure Selection  Here is a link to a Powerpoint on the guidlines to data structures: Powerpoint    "
},
{
  "id": "sec-PatternTutorial",
  "level": "1",
  "url": "sec-PatternTutorial.html",
  "type": "Section",
  "number": "3.4",
  "title": "Pattern Tutorial",
  "body": " Pattern Tutorial  Here is a link to a simple Bridges pattern program: Here   "
},
{
  "id": "sec-PAP",
  "level": "1",
  "url": "sec-PAP.html",
  "type": "Section",
  "number": "4.1",
  "title": "Problems, Algorithms, and Programs",
  "body": " Problems, Algorithms, and Programs   Problems  Programmers regularly encounter three distinct concepts: problems, algorithms, and computer programs.  A problem can be understood as a task that needs to be accomplished, typically described in terms of inputs and desired outputs. It is crucial to define a problem precisely, focusing on inputs and matching outputs, without specifying the solution method. Constraints on the resources allowable for a solution should be included in the problem definition. Every problem solvable by a computer is subject to such constraints, whether explicitly stated or implied. For instance, a computer program can only utilize available main memory and disk space, and it should execute within a reasonable timeframe.  Problems can be seen as mathematical functions. A function matches inputs (domain) with corresponding outputs (range). Inputs can be individual values or collections of information, referred to as parameters. A specific set of parameter values represents an instance of the problem. For example, a sorting function's input parameter might be an array of integers, and a particular array with its size and values at each position would be an instance of the sorting problem. Different instances may yield the same output, but any instance of a problem consistently produces the same output whenever the function is computed with that particular input.  This perspective of problems behaving like mathematical functions may differ from your intuition about how computer programs operate. You may be aware of programs where the same input value produces different outputs on separate occasions. For example, entering the \"date\" command in a typical Linux command line prompt provides the current date, which naturally changes with each day. However, there is more to the input of the \"date\" program than just the command entered. The program computes a function, and on any given day, a properly functioning \"date\" program with a fully specified input will always produce a single answer. The output of any computer program is entirely determined by the program's complete set of inputs. Even a \"random number generator\" is entirely determined by its inputs (although some random number generation systems seem to circumvent this by incorporating random inputs from external physical processes beyond the user's control). The scope of functions implementable by programs falls within the realm of Computability.    Algorithms  An algorithm refers to a method or process employed to solve a problem. If we consider the problem as a function, an algorithm serves as an implementation of that function, transforming an input into the corresponding output. Multiple algorithms can be employed to solve a single problem. Conversely, a specific algorithm is designed to solve only one problem, computing a particular function. The OpenDSA modules cover various problems, and for many of them, we will explore multiple algorithms. In the case of sorting, there are over a dozen well-known algorithms.  Having knowledge of multiple solutions for a problem offers the advantage of selecting the most efficient solution for a particular problem variation or class of inputs. Solution A may be more efficient than solution B for a specific problem variation or input class, while solution B may be superior for another variation or input class. For instance, one sorting algorithm may be ideal for sorting a small collection of integers (which is significant when performing the task multiple times), while another algorithm may excel at sorting a large collection of integers. A third algorithm might be the most suitable for sorting a collection of variable-length strings.  By definition, something can only be considered an algorithm if it possesses the following properties:    Correctness: It accurately computes the desired function, transforming each input into the correct output. Note that every algorithm implements a function, as it maps every input to some output (even if that output is a program crash). The concern here is whether a given algorithm effectively implements the intended function.    Concrete Steps: It comprises a series of well-defined and comprehensible steps that can be executed by the person or machine performing the algorithm. Each step must be accomplishable within a finite amount of time. Thus, the algorithm provides a step-by-step \"recipe\" for problem-solving, where each step is within our capability to perform. The feasibility of executing a step may vary depending on the intended executor. For example, the steps in a cookbook recipe may be sufficiently concrete for instructing a human cook but not for programming an automated cookie-making factory.    Unambiguous Execution: There should be no ambiguity about which step is to be executed next. Typically, algorithms involve selection, such as the use of an \"if\" statement, enabling a choice of the next step. However, at the time of making the choice, the selection process must be unambiguous.    Finite Steps: It consists of a finite number of steps. If the algorithm's description were to comprise an infinite number of steps, it would be impossible to document or implement it as a computer program. Most algorithm description languages incorporate iteration constructs, allowing repeated actions. Programming languages feature constructs like the \"while\" and \"for\" loops to facilitate iteration. Iteration enables concise descriptions, with the actual number of steps executed controlled by the input.    Termination: It must terminate and not enter an infinite loop, ensuring that the algorithm reaches a conclusion.      Programs  We often perceive a computer program as a specific instance or tangible representation of an algorithm in a programming language. Algorithms are typically described in terms of programs or sections of programs. Naturally, multiple programs can be instances of the same algorithm since various modern programming languages can be used to implement the same set of algorithms (although some languages may offer more programmer-friendly features). In practice, the terms \"algorithm\" and \"program\" are often used interchangeably, even though they represent distinct concepts. However, by definition, an algorithm must provide sufficient detail to allow its conversion into a program when necessary.  The requirement for an algorithm to terminate means that not all computer programs align with the technical definition of an algorithm. An example is your operating system. Nonetheless, it is possible to consider the different tasks performed by an operating system, each with its associated inputs and outputs, as individual problems. These problems can be addressed by specific algorithms implemented within different parts of the operating system program. Each algorithm terminates once it produces the desired output for its respective problem.   "
},
{
  "id": "sec-BWAvgCases",
  "level": "1",
  "url": "sec-BWAvgCases.html",
  "type": "Section",
  "number": "4.2",
  "title": "Best, Worst, and Average Cases",
  "body": " Best, Worst, and Average Cases  When analyzing an algorithm, we often contemplate whether to study its best, worst, or average case. Typically, the best case is not of great interest since it tends to occur rarely and may present an overly optimistic depiction of the algorithm's runtime. Analyzing the best case is not usually representative of the algorithm's behavior. However, there are exceptional scenarios where a best-case analysis proves valuable, especially when the best case is highly likely to occur. For instance, the Shellsort and Quicksort algorithms leverage the best-case running time of Insertion Sort to enhance their efficiency.  On the other hand, analyzing the worst case has its advantages as it guarantees a lower bound on the algorithm's performance. This becomes particularly crucial for real-time applications like air traffic control systems. In such cases, it is unacceptable to use an algorithm that performs efficiently for most situations but fails to meet performance requirements when faced with specific scenarios, such as when all airplanes approach from the same direction.  However, for various applications, especially when considering the cumulative cost of executing the program on multiple inputs, analyzing the worst case may not be an accurate measure of the algorithm's overall performance. In such instances, the preferred approach is often to examine the average-case running time. This allows us to understand the algorithm's typical behavior when handling inputs of size n. Nevertheless, conducting an average-case analysis is not always feasible. It requires a thorough understanding of how the actual inputs and their associated costs are distributed among all possible inputs to the program. For example, it was mentioned previously that the average-case performance of the sequential search algorithm involves examining half of the array values. However, this holds true only if the element with value K is equally likely to appear in any position within the array. If this assumption is incorrect, the algorithm may not examine half of the array values on average.  The characteristics of the data distribution significantly impact various search algorithms, including hashing and search trees such as Binary Search Trees (BST). Incorrect assumptions about the data distribution can lead to detrimental effects on a program's space or time performance. Conversely, unusual data distributions can sometimes be leveraged advantageously, as demonstrated by self-organizing lists.  In summary, for real-time applications, a worst-case analysis is often preferred. Otherwise, if sufficient knowledge about the input distribution is available, an average-case analysis is desirable. In the absence of such knowledge, resorting to a worst-case analysis becomes necessary.  "
},
{
  "id": "sec-ComputerorAlgorithm",
  "level": "1",
  "url": "sec-ComputerorAlgorithm.html",
  "type": "Section",
  "number": "4.3",
  "title": "Faster Computer, or Faster Algorithm",
  "body": " Faster Computer, or Faster Algorithm  Imagine you are faced with a problem and you have an algorithm with a running time proportional to n^2, where n represents the size of the input. However, the resulting program takes ten times longer to run than desired. If you were to replace your current computer with a new one that is ten times faster, would the n^2 algorithm become acceptable? It might seem logical that the faster computer would allow you to complete your work quickly enough, even with an algorithm that has a high growth rate, as long as the problem size remains the same. But here's an interesting observation: when most people acquire a faster computer, they don't necessarily run the same problem faster; they tend to tackle a larger problem instead. For example, on your old computer, you might have been content sorting 10,000 records during your lunch break. With the new, faster computer, you might aim to sort 100,000 records in the same time. Since you won't finish your lunch any sooner, it makes sense to solve a larger problem. Additionally, due to the increased speed of the new machine, you would ideally like to sort ten times as many records.  If the algorithm's growth rate is linear, meaning the equation describing the running time on input size n is T(n) = cn for some constant c, then sorting 100,000 records on the new machine would take the same amount of time as sorting 10,000 records on the old machine. However, if the algorithm's growth rate exceeds cn, such as c1n^2, then it won't be possible to solve a problem ten times the size in the same amount of time on a machine that is only ten times faster.  "
},
{
  "id": "sec-AALecture",
  "level": "1",
  "url": "sec-AALecture.html",
  "type": "Section",
  "number": "4.4",
  "title": "Algorithm Analysis Lecture",
  "body": " Algorithm Analysis Lecture  Here is the link to download the lecture: Slides   "
},
{
  "id": "sec-ComplexityMatters",
  "level": "1",
  "url": "sec-ComplexityMatters.html",
  "type": "Section",
  "number": "4.5",
  "title": "Complexity Matters",
  "body": " Complexity Matters  Here is a link to an assignment to help you understand Big-Oh notation: Here   "
},
{
  "id": "sec-Pointers",
  "level": "1",
  "url": "sec-Pointers.html",
  "type": "Section",
  "number": "5.1",
  "title": "Pointers Syntax",
  "body": " Pointers Syntax   Syntax  The fundamental concepts of references, pointees, dereferencing, and assigning discussed earlier are all that you require to construct reference code. However, when discussing reference code, we need to employ a familiar syntax, which may not be particularly engaging. In this case, we will utilize the Java language syntax, which offers the advantage of influencing the syntaxes of numerous other programming languages.    Declaring a Reference Variable  To establish a reference to a non-primitive data type (i.e., an object), one must declare a variable of that specific object's type. Similar to other variables, reference variables are declared in the same manner. The declaration includes specifying the type and identifier of the new variable, and memory is allocated to store its value. However, it is crucial to note that declaring a reference variable does not automatically assign a pointee (object) to the reference. Initially, the reference holds a \"bad\" value until it is assigned to an actual object.    Reference Rules  The rules governing reference structures remain concise, regardless of their complexity. A reference variable serves as a container for storing a reference to its associated pointee. The pointee itself holds valuable information or data. To access the content of a reference, the dereference operation is used. However, it is crucial to note that a reference can only be dereferenced once it has been assigned to a valid pointee. Many issues or errors related to references occur when this rule is violated. Allocating a reference does not automatically assign it to refer to a specific pointee. The act of assigning the reference to a particular pointee is a distinct operation that can sometimes be overlooked. When two references are assigned to each other, they start referring to the same pointee. This allows for object sharing, enabling multiple references to access and manipulate the same underlying object.    Java Reference Vs Pointers  Java references possess two key features that distinguish them from the less permissive pointers found in languages like C or C++.    Reduced bugs: Java's implementation of reference manipulation is accurate and automatic, resulting in fewer common reference-related bugs. Additionally, the Java runtime system verifies each reference value whenever it is used. As a result, if a null reference is dereferenced, the system promptly detects it on the line where the issue occurs. This stands in contrast to languages like C++ where dereferencing a null value might not cause a program crash until later stages. This ability to pinpoint problems immediately can greatly enhance a programmer's productivity by identifying the exact location of an issue.    Slower execution: Due to the extensive runtime implementation of reference management and the additional runtime checking, Java code tends to run slower compared to languages like C and C++. However, the advantages of improved programmer efficiency and reduced bugs make the trade-off of slower execution worthwhile for many applications.     "
},
{
  "id": "sec-LinkedNodes",
  "level": "1",
  "url": "sec-LinkedNodes.html",
  "type": "Section",
  "number": "5.2",
  "title": "Linked Nodes",
  "body": " Linked Nodes  In this module, we will introduce the concept of a link node. A link node consists of a value field and a pointer to another link node. In subsequent lessons, you will delve into linked lists, which are constructed using link nodes. However, for now, we will utilize link nodes as a straightforward means to connect various objects together.  "
},
{
  "id": "sec-DoubleLL",
  "level": "1",
  "url": "sec-DoubleLL.html",
  "type": "Section",
  "number": "5.3",
  "title": "Doubly Linked Lists",
  "body": " Doubly Linked Lists  The singly linked list allows direct access from a list node solely to the next node in the list. On the other hand, a doubly linked list offers the convenience of accessing both the next and preceding nodes from a list node. This is achieved by including two pointers in each doubly linked list node: one pointing to the next node (similar to the singly linked list) and another pointing to the preceding node. The primary advantage of using a doubly linked list is its ease of implementation compared to a singly linked list. Although the code for the doubly linked implementation may be slightly longer than its singly linked counterpart, it tends to be more straightforward in its purpose, making it easier to implement and debug. It is important to note that the decision to use a doubly or singly linked list should be hidden from the user of the List class.  Similar to the implementation of our singly linked list, the doubly linked list implementation incorporates a header node. Additionally, we introduce a tailer node at the end of the list. The tailer node, like the header node, does not contain any value and is always present. During the initialization of the doubly linked list, the header and tailer nodes are created. The data member \"head\" points to the header node, and \"tail\" points to the tailer node. These nodes simplify the insert, append, and remove methods by eliminating the need for special-case code when the list is empty or when inserting at the head or tail of the list.  "
},
{
  "id": "sec-LLLecture",
  "level": "1",
  "url": "sec-LLLecture.html",
  "type": "Section",
  "number": "5.4",
  "title": "Linked List Lecture",
  "body": " Linked List Lecture  Here is the link to download the lecture on linked lists: Slides   "
},
{
  "id": "sec-LLTutorial",
  "level": "1",
  "url": "sec-LLTutorial.html",
  "type": "Section",
  "number": "5.5",
  "title": "Linked List Tutorial",
  "body": " Linked List Tutorial  Here is a link to a tutorial on building a linked list: Here   Here is a tutorial on styling a linked list of earthquake records: Here   "
},
{
  "id": "sec-Exercises",
  "level": "1",
  "url": "sec-Exercises.html",
  "type": "Section",
  "number": "5.6",
  "title": "Exercises",
  "body": " Exercises   Linked Lists Vs Array Lists Exercise 1  The assignment is to log the performance of insertion\/deletion operations on ArrayLists and LinkedLists. Complete each unfinished function that has a TODO comment. Please provide comments on how your code works. Here is the main file: main.java   This project uses the built in ArrayList and LinkedList classes in Java.  You can refer to the ArrayList docs: Here   You can refer to the LinkedList docs: Here     Linked Lists Vs Array Lists Exercise 2  Please download this word document: Data_Structure_Operations.docx   Fill out each cell with the BigO worst case runtime complexity of each operation for the given data structure. Also, write a couple sentences describing why each operation is the runtime you have choosen.   "
},
{
  "id": "colophon-2",
  "level": "1",
  "url": "colophon-2.html",
  "type": "Colophon",
  "number": "",
  "title": "Colophon",
  "body": " This book was authored in PreTeXt .  "
}
]

var ptx_lunr_idx = lunr(function () {
  this.ref('id')
  this.field('title')
  this.field('body')

  ptx_lunr_docs.forEach(function (doc) {
    this.add(doc)
  }, this)
})
